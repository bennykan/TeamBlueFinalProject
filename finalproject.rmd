---
title: "Property Value Prediction Using Clustering and Linear Regression - Automated Valuation Model (AVM)"
author: "Tyler Blakeley, Benjamin Kan, Mohammad Islam, Avijeet Singh"
date: "November 16 2018"
output:
  html_document:
    fig_height: 4.5
    fig_width: 7
    highlight: tango
    number_sections: yes
    theme: readable
    toc: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```

# Business Understanding

We are the owners of a start-up company based in New York City who specializes in appraisal management and national real estate information, servicing major banks, lenders, mortgage insurers, credit unions, and independent mortgage brokers. Currently, most of the real estate transactions including mortgage loans and commercial property acquisitions rely on appraisers to estimate the property values. However, this could take at least days to complete, which the mortgage lenders and brokers think takes too long given the hyper competitive nature of the real estate industry. Moreover, the property valuation process is at times manual without comprehensive substantiation of how the appraisers come up with the values. Therefore, we reckon that the real estate appraisal industry is ripe for disruption. Our product is a property value prediction model which can be used by mortgage lenders and brokers to value the properties within minutes.  It is a national solution built using highly advanced machine learning algorithms, which can take into account extensive and diverse data sets, learn from the data in their environment, as well as incorporate new and different situations, as the market evolves.           

# Data Understanding

## Data Source and Collection

We sourced the NYC property sales data from Kaggle (https://www.kaggle.com/new-york-city/nyc-property-sales/home). This dataset contains the properties sold in NYC over a 12-month period from September 2016 to September 2017. The dataset contains 22 attributes including the precise property locations (Borough, Neighborhood, Block, Zip Code and Address), building class, property types (Residential or Commercial), property size and tax class. The dataset consists of about 84.5K of property sale transactions.

## Data Description

In the dataset, there are 22 variables. A detailed description of these variables can be found at https://www1.nyc.gov/assets/finance/downloads/pdf/07pdf/glossary_rsf071607.pdf

Variable Name                   | Description
--------------------------------|---------------------------------------------------------------------------
 #                              | ID of the property
BOROUGH                         | The name of the borough in which the property is located                    
NEIGHBORHOOD                    | Neighborhood name in the course of valuing properties
BUILDING CLASS CATEGORY         | Building class descriptions
TAX CLASS                       | 1: Residential property up to three units; 2: All other property that is                                    | primarily residential (Co-ops and condos) 3: Property with equipment owned                                  | by a gas, telephone or electric company; 4: All other properties not                                        | included  
BLOCK                           | Sub-division of the borough on which the real properties are located
LOT                             | Subdivision of a block and represents the property unique location     
EASEMENT                        | A right which allows an entity to make limited use of another's real                                        | property
BUILDING CLASS AT PRESENT       | Building Code. Details can be found at
                                | https://www1.nyc.gov/assets/finance/jump/hlpbldgcode.html
ADDRESS                         | Street address of the property as listed on the Sales File. Coop sales                                                               | include the apartment number
ZIP CODE                        | The property's postal code
RESIDENTIAL UNITS               | The number of residential units at the listed property
COMMERCIAL UNITS                | The number of commercial units at the listed property
LAND SQUARE FEET                | The land area of the property listed in square feet
GROSS SQUARE FEET               | The total area of all the floors of a building as measured from the exterior                                 | surfaces of the outside walls of the building
YEAR BUILT                      | Year the structure on the property was built
BUILDING CLASS AT TIME OF SALE  | The Building Classification is used to describe a propertyâ€™s constructive                                   | use. The first position of the Building Class is a letter that is used to                                   | describe a general class of properties
SALES PRICE                     | Price paid for the property
SALE DATE                       | Date of property sold

Note: $0 Sales Price: A $0 sale indicates that there was a transfer of ownership without a cash consideration. There can be a number of reasons for a $0 sale including transfers of ownership from parents to children. 

## Data Exploration

### Load Packages

```{r, message=FALSE,warning=FALSE}
#import packages;
library(dplyr)
library(reshape2)
library(ggplot2)
library(Hmisc)
library(corrplot)
library(mice)
library(VIM)
library(pROC)
library(caret)
library(corrgram)
library(GGally)
library(ggthemes) 
library(DMwR)
library(gridExtra)
library(rattle)
library(readxl)
library(cluster)
library(sqldf)
library(knitr)
library(xgboost)
library(outliers)
```

### Load Datasets

Now that the packages are loaded, we can load in the dataset. 

```{r, message=FALSE,warning=FALSE}
data=read.csv(file.choose(), header = TRUE, na.strings = c("NA","","#NA"," -  "))
```

Now let us see how our dataset looks like:

```{r, message=FALSE,warning=FALSE}
str(data)
```

From the above we can see that 1st column has only the observation numbers and is not necessary for our data exploration so i will remove it for the time being and make a new dataset.

```{r, message=FALSE,warning=FALSE}
nyc_data<-as_data_frame(data[,-1])
```

##Filter For Residential Data

Based on the Business case we only want to create a model for Residentential homes.  We will remove any non residenttial homes from the BUILDING.CLASS.CATEGORY and reduce the dimenionality of the data by grouping condo's and coops under one level.


```{r, message=FALSE,warning=FALSE}
data_out <- data

#Remove any home with facilities in its class
data_out <- data_out[!data_out$BUILDING.CLASS.CATEGORY %like% 'FACILITIES',]

#Remove any commerical
data_out <- data_out[!data_out$BUILDING.CLASS.CATEGORY %like% 'COMMERCIAL',]

#Remove office
data_out <- data_out[!data_out$BUILDING.CLASS.CATEGORY %like% 'OFFICE',]


#Remove HOTEL
data_out <- data_out[!data_out$BUILDING.CLASS.CATEGORY %like% 'HOTEL',]

#Remove Vacant Land
data_out <- data_out[!data_out$BUILDING.CLASS.CATEGORY %like% 'VACANT LAND',]

#Remove Other
data_out <- data_out[!data_out$BUILDING.CLASS.CATEGORY %like% 'OTHER',]

#Remove PARK
data_out <- data_out[!data_out$BUILDING.CLASS.CATEGORY %like% 'PARK',]

#Remove WAREHOUSE
data_out <- data_out[!data_out$BUILDING.CLASS.CATEGORY %like% 'WAREHOUSE',]

#Remove STORE
data_out <- data_out[!data_out$BUILDING.CLASS.CATEGORY %like% 'STORE',]

#Remove FACTORY
data_out <- data_out[!data_out$BUILDING.CLASS.CATEGORY %like% 'FACTOR',]


#Remove ASYLUM
data_out <- data_out[!data_out$BUILDING.CLASS.CATEGORY %like% 'ASYLUM',]

#Remove RENTAL
data_out <- data_out[!data_out$BUILDING.CLASS.CATEGORY %like% 'RENTAL',]


data_out$BUILDING.CLASS.CATEGORY <- as.character(data_out$BUILDING.CLASS.CATEGORY)
data_out$BUILDING.CLASS.CATEGORY <- ifelse(data_out$BUILDING.CLASS.CATEGORY %like% 'CONDO', "CONDO",data_out$BUILDING.CLASS.CATEGORY)
data_out$BUILDING.CLASS.CATEGORY <- ifelse(data_out$BUILDING.CLASS.CATEGORY %like% 'COOP', "COOP",data_out$BUILDING.CLASS.CATEGORY)

data_out$BUILDING.CLASS.CATEGORY <- as.factor(data_out$BUILDING.CLASS.CATEGORY)

```


Now we will introduce a new column which will only give me the BUILDING CLASS CATEGORY i.e "walkup aprartments","elevator apartments" etc. and not the numbers in front of them as they are not needed.
First i will remove the first three characters and check if it worked.

```{r, message=FALSE,warning=FALSE}
nyc_data$BUILDING.CLASS.CATEGORY<-substring(nyc_data$BUILDING.CLASS.CATEGORY,3)
head(nyc_data$BUILDING.CLASS.CATEGORY)
```

Instead of the numbers in the BOROUGH column we will put in the names:

```{r, message=FALSE,warning=FALSE}

data_out$BOROUGH <- ifelse(data_out$BOROUGH==1,'Manhattan',data_out$BOROUGH)
data_out$BOROUGH <- ifelse(data_out$BOROUGH==2,'Bronx',data_out$BOROUGH)
data_out$BOROUGH <- ifelse(data_out$BOROUGH==3,'Brooklyn',data_out$BOROUGH)
data_out$BOROUGH <- ifelse(data_out$BOROUGH==4,'Queens',data_out$BOROUGH)
data_out$BOROUGH <- ifelse(data_out$BOROUGH==5,'Staten Island',data_out$BOROUGH)


```

We will remove the column "EASE-MENT" as it has null value throughout:

```{r, message=FALSE,warning=FALSE}
nyc_data$EASE.MENT<-NULL
```

Now let us introduce a new column "Building Age" as it is much simpler to understand.

```{r, message=FALSE,warning=FALSE}
nyc_data$BUILDING.AGE<- 2018-nyc_data$YEAR.BUILT
```

Let us finally check for duplicates and remove if there are any:

```{r, message=FALSE,warning=FALSE}
data_out %>% filter(duplicated(data_out) == TRUE) %>% nrow()
nyc_data %>% filter(duplicated(nyc_data) == TRUE) %>% nrow()
```
From here we can see that there are 765 duplicates and we will remove the same.

Removing duplicates:

```{r, message=FALSE,warning=FALSE}
data_out <- unique(data_out)
nyc_data <- unique(nyc_data)
```

Getting rid of NAs in the Sale price data as we need to take mean of the sale price in our data analysis.

```{r, message=FALSE,warning=FALSE}
nyc_data <- nyc_data[!is.na(nyc_data$SALE.PRICE),]
nyc_data <- nyc_data[nyc_data$SALE.PRICE != 0,]
```

###Data Type conversions
The original dataset has columns with types that are not suitable for analysis so we will convert them.
The Boroughs,building class category,zipcode,tax class at the time of sale  are converted to factors.
Address,apartment number converted to chr.
Land square feet,gross square feet,year to numeric.
Sale date to date.

```{r, message=FALSE,warning=FALSE}
fac<-c(1,3,10,17)
nyc_data[fac]<-lapply(nyc_data[fac],factor)
chr<-c(8,9)
nyc_data[chr]<-lapply(nyc_data[chr],as.character)
num<-c(14,15)
nyc_data[num]<-lapply(nyc_data[num],as.numeric)
dt<-c(20)
nyc_data[dt]<-lapply(nyc_data[dt],as.Date)

```

No let us begin our data exploration.

###Sale Price
First let us see the summary of sale price and then if the data is skewed or not using the quantile function.

```{r, message=FALSE,warning=FALSE}
summary(nyc_data$SALE.PRICE)
quantile(nyc_data$SALE.PRICE, probs = seq(from = 0, to = 1, by = .1))
```

So from the above we can see that there are many values with low sale price.We will check for sale price values that are less than a 1000.

```{r, message=FALSE,warning=FALSE}
nyc_data %>% filter(SALE.PRICE <= 1000) %>% nrow()
```

We will remove these rows from our data and check the distribution of the data again.

```{r, message=FALSE,warning=FALSE}
nyc_data <- nyc_data %>% filter(!SALE.PRICE <= 1000)
quantile(nyc_data$SALE.PRICE, probs = seq(from = 0, to = 1, by = .1))
```

This shows better distribution with the very low outliers removed. The highest sale price being $2,210,000,000.


###Boroughs
Let us see which borough recorded the the most property sales and which borough is the most expensive by avg. sale price.
```{r, message=FALSE,warning=FALSE}
p1 <- ggplot(data = nyc_data, aes(x = `BOROUGH`)) +
  geom_bar() +
  ggtitle("In-Demand Borough in NYC") +
  scale_y_continuous("# of Property Sales", labels = scales::comma) +
  scale_x_discrete("Borough")

p2 <- ggplot(data = nyc_data, aes(x = `BOROUGH`, y = mean(`SALE.PRICE`))) +
  geom_bar(stat = "identity") +
  ggtitle("Most Expensive Borough in NYC", subtitle = "Avg. Property Sale Price in NYC") +
  scale_y_continuous("Avg Sale Price", labels = scales::dollar) +
  scale_x_discrete("Borough")

grid.arrange(p1, p2, ncol = 1)
```

The plots show that queens had the most number of property sales followed by brooklyn. The average sale price of a property in queens is the highest followed by brooklyn.

###Neigborhoods
Now that we have explored the sale price according to the boroughs let us check for the most expensive neighborhoods.
We will only take the top ten neigborhoods as it will be difficult to plot all the neighborhoods.
```{r, message=FALSE,warning=FALSE}
dfnyc1<-as.data.frame(table(nyc_data$BOROUGH, nyc_data$NEIGHBORHOOD))
names(dfnyc1) <- c('BOROUGH','NEIGHBORHOOD', 'Freq')
dfnyc1 <- dfnyc1 %>% arrange(desc(Freq)) %>% head(10)
p3<-ggplot(data = dfnyc1,aes(x=dfnyc1$NEIGHBORHOOD,y=dfnyc1$Freq,fill=dfnyc1$BOROUGH))+geom_bar(stat = "identity")+scale_y_continuous("Number of sales")+scale_x_discrete("Neighborhood")+coord_flip()

dfnyc2<- 
  nyc_data %>% group_by(BOROUGH, NEIGHBORHOOD) %>% 
  summarise(MeanSP = mean(`SALE.PRICE`)) %>% 
  arrange(desc(MeanSP)) %>% head(10)

p4 <- ggplot(data = dfnyc2, aes(x = dfnyc2$NEIGHBORHOOD, y = dfnyc2$MeanSP, fill = dfnyc2$BOROUGH)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  ggtitle("Most Expensive Neighborhoods in NYC", 
          subtitle = "Top Neighborhoods by Avg Price") +
  scale_y_continuous("Avg Sale Price", labels = scales::dollar) +
  scale_x_discrete("Neighborhood") 

grid.arrange(p3,p4,ncol=1)
```

From the above graphs we can see that the most in-demand neighborhood is in Queens and the most expensive neighborhood by avg. price is in Staten island. 

###Buildings
Let us explore the top ten most in-demand building class and the most expensive building class and to what borough do they belong to.
```{r, message=FALSE,warning=FALSE}
dfnyc3<-as.data.frame(table(nyc_data$BOROUGH, nyc_data$BUILDING.CLASS.CATEGORY))
names(dfnyc3) <- c('BOROUGH','BUILDING CLASS CATEGORY', 'Freq')
dfnyc3 <- dfnyc3 %>% arrange(desc(Freq)) %>% head(10)
ggplot(data = dfnyc3,aes(x=dfnyc3$BOROUGH,y=dfnyc3$Freq,fill=dfnyc3$`BUILDING CLASS CATEGORY`))+geom_bar(stat = "identity",position = "dodge")+scale_y_continuous("Number of sales")+scale_x_discrete("Borough")


```

From the above diagram it is clear that the one family dwellings are very popular in queens and staten island.
Condos are particulary in demand in manhattan and in Brooklyn the two family dwellings are more popular than the rest.
Now lets take a look at the most expensive building class.

```{r, message=FALSE,warning=FALSE}
dfnyc4<- 
  nyc_data %>% group_by(BOROUGH,BUILDING.CLASS.CATEGORY ) %>% 
  summarise(MeanSP = mean(`SALE.PRICE`)) %>% 
  arrange(desc(MeanSP)) %>% head(10)

ggplot(data = dfnyc4, aes(x = dfnyc4$BUILDING.CLASS.CATEGORY, y = dfnyc4$MeanSP, fill = dfnyc4$BOROUGH)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  ggtitle("Most Expensive Buildings in NYC") +
  scale_y_continuous("Avg Sale Price", labels = scales::dollar) +
  scale_x_discrete("Building Type") + theme(legend.position = "bottom")

```
From the above graph we can see that the most expensive buildings are the luxury hotels in Manhattan.


###Land Square feet across Boroughs:
First we will see how the data looks like:

```{r, message=FALSE,warning=FALSE}
summary(nyc_data$LAND.SQUARE.FEET)
```

Let us check for outliers

```{r, message=FALSE,warning=FALSE}
outlier(nyc_data$LAND.SQUARE.FEET)
```


Let us try to find out how the sale price is per land sq. feet.
First we will check for NA's in the Land square feet attribute:

```{r, message=FALSE,warning=FALSE}
sum(is.na(nyc_data$LAND.SQUARE.FEET))
```

We can see that there are 21000 instances with NA as value and we will remove the same as we just want to find the relation between the square footage and the sale price. For this purpose only we will create a new data frame "nyc_data_sqfeet" and we will plot the graphs using this dataframe.

```{r, message=FALSE,warning=FALSE}
nyc_data_sqfeet <- as_data_frame(nyc_data[!is.na(nyc_data$LAND.SQUARE.FEET),])
```

Now that we have created the new data frame let us check for the relation between the square footage and the sale price.

```{r, message=FALSE,warning=FALSE}
ggplot(data = nyc_data_sqfeet, aes(x = log(nyc_data_sqfeet$LAND.SQUARE.FEET), y = log(nyc_data_sqfeet$SALE.PRICE), color = `BOROUGH`)) +
  geom_jitter() +
  geom_smooth(method = "lm") +
  theme(legend.position = "bottom") +
   ggtitle("Price Vs Land Square Footage in NYC", 
          subtitle = "Distribution of Sale Price vs Land Square feet Borough-wise") +
  scale_y_continuous("Property Sale Price", labels = scales::dollar) +
  scale_x_continuous("Land Square Footage", labels = scales::comma) +
   facet_wrap(~ BOROUGH) 
```

We can see that the sale price increases with the square footage across all boroughs.



###Gross Square feet across Boroughs:
First we will see how the data looks like:

```{r, message=FALSE,warning=FALSE}
summary(nyc_data$GROSS.SQUARE.FEET)
```

Let us check for outliers

```{r, message=FALSE,warning=FALSE}
outlier(nyc_data$GROSS.SQUARE.FEET)
```

Let us try to find out how the sale price is per gros sq. feet.
First we will check for NA's in the gross square feet attribute:

```{r, message=FALSE,warning=FALSE}
sum(is.na(nyc_data$GROSS.SQUARE.FEET))
```

We can see that there are 21532 instances with NA as value and we will remove the same as we just want to find the relation between the square footage and the sale price. For this purpose only we will create a new data frame "nyc_data_grosssqfeet" and we will plot the graphs using this dataframe.

```{r, message=FALSE,warning=FALSE}
nyc_data_grosssqfeet <- as_data_frame(nyc_data[!is.na(nyc_data$GROSS.SQUARE.FEET),])
```

Now that we have created the new data frame let us check for the relation between the square footage and the sale price.

```{r, message=FALSE,warning=FALSE}
ggplot(data = nyc_data_grosssqfeet, aes(x = log(nyc_data_grosssqfeet$GROSS.SQUARE.FEET), y = log(nyc_data_grosssqfeet$SALE.PRICE), color = `BOROUGH`)) +
  geom_jitter() +
  geom_smooth(method = "lm") +
  facet_wrap(~ BOROUGH) +
  ggtitle("Price Vs Gross Square Footage in NYC",
          subtitle = "Distribution of Sale Price vs Gross Square feet Borough-wise") +
  scale_y_continuous("Property Sale Price", labels = scales::dollar) +
  scale_x_continuous("Gross Square Footage", labels = scales::comma) +
  theme(legend.position = "bottom")
```

We can see a similar trend as we saw with the land square footage. The sale price increases with the gross square footage.


###Building Age
```{r, message=FALSE,warning=FALSE}
ggplot(data = nyc_data, aes(x = nyc_data$BUILDING.AGE, y = log(nyc_data$SALE.PRICE))) +
  geom_point(aes(col = nyc_data$BOROUGH)) +
  geom_smooth(method = "lm") +
  theme(legend.position = "bottom") +
  ggtitle("Price of Oldest Properties sold in NYC", subtitle = "Oldest buildings in NYC") +
  scale_y_continuous("Property Value Distribution", labels = scales::dollar) +
  scale_x_continuous("Building Age") 
```
So we can see from the above graph that the building age is not a good predictor of Sale price.


#Check current variable type

```{r, message=FALSE,warning=FALSE}
str(data_out)

#Change the Borough to Categorical

data_out$BOROUGH <- as.factor(data_out$BOROUGH)

#Change ZIP.CODE to categorical

data_out$ZIP.CODE <- as.factor(data_out$ZIP.CODE)

#Change Sale.DATE to date 

data_out$SALE.DATE <- as.Date(data_out$SALE.DATE)

str(data_out)

```


###Removed Missing and Outlier Data from SALE.PRICE

To remove outliers values we will use the box and whisker plot.

```{r, message=FALSE,warning=FALSE}

summary(data_out)



#Remove sale price of 0 and NA's
data_out <- data_out[data_out$SALE.PRICE != 0,]
data_out <- data_out[!is.na(data_out$SALE.PRICE),]

data_rm_outliers = data_out;

remove_outliers <- function(x,lbound) {
  qnt <- quantile(x, probs=c(.25, .75), na.rm = TRUE)
  H <- 1.5 * IQR(x, na.rm = TRUE)
  y=x
  y[x < (qnt[1] - H/lbound)] = -999999
  y[x > (qnt[2] + H)] = -999999
  y
  
}

#identify Outliers for SALE.PRICE, GROSS.SQUARE.FEET and LAND.SQAURE.FEET
data_out$SALE.PRICE <- remove_outliers(data_rm_outliers$SALE.PRICE,4)



data_out <- data_out[data_out$SALE.PRICE!=-999999,]



```

We have removd the missing and outlier SALE.PRICE data.  When we were removing outliers we noticed that it was not removing any small values of SALE.PRICE, this was due to the fact that the minium whisker was going negative and there is no sale price that is below zero.  We made a small change where we divide the inter quartile range by 4 for the minium whisker to remove some small values of sale.price.

We have reduced our dataset to 46220 oberservations.


###Removed Missing and Outlier Data from Gross.SQuare.FEET, YEAR.BUILT and LAND.SQUARE.FEET


####LAND.SQUARE.FEET

We noticed there is zero values for LAND.SQUARE.FEET, we want to investigate is this an error or are there some type of properties that have zero lot size. Specfically we want to look at the what is the LAND.SQUARE.FEET for condos.


```{r, message=FALSE,warning=FALSE}
#Investigate Land square feet

sqldf('SELECT [BUILDING.CLASS.CATEGORY],COUNT(*) FROM data where [LAND.SQUARE.FEET] = 0 group by [BUILDING.CLASS.CATEGORY]')


```

We can see that the only type of houses with 0 LAND.SQUARE.FEET are condos and this does not seem to be an error and requires no imputation.


####GROSS.SQUARE.FEET

For GROSS.SQUARE.FEET we replaced any 0 values with NA, our assumaption is that no house can have 0 living area.


```{r, message=FALSE,warning=FALSE}

data_out$GROSS.SQUARE.FEET[data_out$GROSS.SQUARE.FEET == 0] <- NA

```

####YEAR.BUILT

For YEAR.BUILT we replace any house with the year built before 1776 to NA's.

```{r, message=FALSE,warning=FALSE}

data_out$YEAR.BUILT[data_out$YEAR.BUILT <= 1776] <- NA

```



###Impute Data


```{r, message=FALSE,warning=FALSE}

#Analyzed Missing values

aggr_plot = aggr(data_out, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(data_out), cex.axis=.7, gap=3, ylab=c("Histogram of missing data","Pattern"))


#remove easement columns all NA's and TOTAL.UNITS
data_out <- data_out[,!colnames(data_out) %in% c('EASE.MENT','X','TOTAL.UNITS')]

summary(data_out)



borough_list <- c('Manhattan','Queens','Bronx','Brooklyn','Staten Island')

for (i in borough_list) {
Gross <- median(data_out$GROSS.SQUARE.FEET[data_out$BOROUGH==i & !is.na(data_out$GROSS.SQUARE.FEET)])
data_out$GROSS.SQUARE.FEET[data_out$BOROUGH==i & is.na(data_out$GROSS.SQUARE.FEET)] <- Gross

Land <- median(data_out$LAND.SQUARE.FEET[data_out$BOROUGH==i & !is.na(data_out$LAND.SQUARE.FEET)])
data_out$LAND.SQUARE.FEET[data_out$BOROUGH==i & is.na(data_out$LAND.SQUARE.FEET)] <- Land

Year_Built <- median(data_out$YEAR.BUILT[data_out$BOROUGH==i & !is.na(data_out$YEAR.BUILT)])
data_out$YEAR.BUILT[data_out$BOROUGH==i & is.na(data_out$YEAR.BUILT)] <- Year_Built
}
summary(data_out$GROSS.SQUARE.FEET)
summary(data_out$LAND.SQUARE.FEET)
summary(data_out$YEAR.BUILT)

```

We inputed the 3 columns that had some missing values and we dropped the column EASE.MENT because it had 100% missing values.  We inputed the values by taking the median value for the variable for the borough it is in,


###Remove outliers for GROSS.SQUARE.FEET AND LAND.SQUARE.FEET


```{r, message=FALSE,warning=FALSE}
data_rm_outliers <- data_out

data_out$GROSS.SQUARE.FEET <- remove_outliers(data_rm_outliers$GROSS.SQUARE.FEET,4)
data_out$LAND.SQUARE.FEET <- remove_outliers(data_rm_outliers$LAND.SQUARE.FEET,4)

data_out <- data_out[data_out$GROSS.SQUARE.FEET!=-999999&data_out$LAND.SQUARE.FEET!=-999999,]

```


###Check Variable Distribution


####SALE.PRCIE

Want to Check if the distrbution of SALE.PRICE is normal, if not coverted to normal through transformation

```{r, message=FALSE,warning=FALSE}


plot(density(data_out$SALE.PRICE))

#Use log transformation to change to normal distrubution

plot(density(sapply(data_out$SALE.PRICE, log10)))

#Update SALE.PRICE

data_out$SALE.PRICE <- sapply(data_out$SALE.PRICE, log10)
```


#Modelling


##Encoding Categorical Variables

To Encode the categorical variables we chose to replace each level of each variable with the average SALE.PRICE.


```{r, message=FALSE,warning=FALSE}
encod_Categories <- function (x,y) {
  z = y 
  for (i in levels(x)) {
    encod_value <- median(y[x==i])
    z[x==i] <-  encod_value
    }
  z
  
}

#Find all factor variables to encod

is.fact <- cbind('BOROUGH','NEIGHBORHOOD','BUILDING.CLASS.CATEGORY','TAX.CLASS.AT.PRESENT','ZIP.CODE','BUILDING.CLASS.AT.TIME.OF.SALE')

encod_data <- data_out[,is.fact]
for (i in 1:NCOL(encod_data)){
  encod_data[,i] <- encod_Categories(encod_data[,i],data_out$SALE.PRICE)
  
}

#Confirm all variables are numeric
str(encod_data)

table(encod_data$BUILDING.CLASS.CATEGORY,data_out$BUILDING.CLASS.CATEGORY)

```

We can see that all the columns are now numeric, each level has been replaced with the median sale price value for that level.


##Create dataset for modelling

Now that we have all the categorical variables converted to numeric,we can create a modelling dataset that includes date field, initall numeric columns and encoded numeric columns.

```{r, message=FALSE,warning=FALSE}

is.num <- sapply(data_out, is.numeric)


data_model <- (data.frame((data_out[,is.num]),encod_data,data_out$SALE.DATE))

#Remove columns 

data_model <- data_model[,!colnames(data_model)%in%c('TAX.CLASS.AT.PRESENT','RESIDENTIAL.UNITS','COMMERCIAL.UNITS','TAX.CLASS.AT.TIME.OF.SALE','BUILDING.CLASS.AT.TIME.OF.SALE')]



```

##Look at Correalation

```{r, message=FALSE,warning=FALSE}
cor_result=rcorr(as.matrix(data_model[,1:10]))


# ++++++++++++++++++++++++++++
# flattenCorrMatrix - makes it easier to read (in my opinion)
# ++++++++++++++++++++++++++++
flattenCorrMatrix <- function(cormat, pmat) {
  ut <- upper.tri(cormat)
  data.frame(
    row = rownames(cormat)[row(cormat)[ut]],
    column = rownames(cormat)[col(cormat)[ut]],
    cor  =(cormat)[ut],
    p = pmat[ut]
  )
}

#Simple method to flatten (if that how you want to look at it)
cor_result_flat = flattenCorrMatrix(cor_result$r, cor_result$P)
cor_result_flat[cor_result_flat$cor >0.9,]
cor_result_flat[abs(cor_result_flat$cor) <0.1&cor_result_flat$column=='SALE.PRICE',]


```




## Split into Training & Test Sets

We split the data into 80% training data and 20% testing data, the data is balanced on the Borough variable to ensure each data is representative of the initial data set.

```{r, message=FALSE,warning=FALSE}
#set.seed(456292)

#Create index to split data
train_set <- createDataPartition(data_model$BOROUGH, p = 0.8, list = FALSE)



#Create train and test data
train_data <- data_model[train_set,]
test_data <- data_model[-train_set,]


```

## Build Model 

We decided to implement a linear regresion model.  With linear regression the goal is to identify a line that best first the data.  The line of best fit is the one which the prediction error is the least.

```{r, message=FALSE,warning=FALSE}
#reduce bias using cross validation
folds=10

fitControl <- trainControl(method="cv",number=folds)

#Implement Linear Regresion Model (lm)
LM_model <- train(SALE.PRICE~., data=train_data, method="lm",
metric = "MAE",
trControl = fitControl)


```

We chose to run the linear regression intially on all of our variables.  We will now investigate if there is some variables we can remove that are of low signifiance.

## Assess Model



###variable Significance

```{r, message=FALSE,warning=FALSE}

summary(LM_model)


print(LM_model)


```

All of the variables have low p-values, and they are all significants.

### Accuracy of Linear regression model

```{r, message=FALSE,warning=FALSE}

# Predict Test results
LM_model.pred <- predict(LM_model, test_data)


actuals_preds <- data.frame(cbind(actuals=test_data$SALE.PRICE, predicteds=LM_model.pred )) 

actuals_preds <- sapply(actuals_preds, function(x) 10^x)
# Check Accuracy of Model 
min_max_accuracy <- mean (apply(actuals_preds, 1, min) / apply(actuals_preds, 1, max))
min_max_accuracy
mape <- mean(abs((actuals_preds[,"predicteds"]  - actuals_preds[,"actuals"]))/actuals_preds[,"actuals"])

mape

#Within 10%


Within10 <- ifelse(abs((actuals_preds[,"predicteds"] - actuals_preds[,"actuals"]))/actuals_preds[,"actuals"]>0.1,0,1)

basic_LM_Accuracy <- mean(Within10)

basic_LM_Accuracy


```

The linear regression has an intial accuracy of `r ROUND(basic_LM_Accuracy,4) % ` within 10% of actual value.


####Check Residual of Linear Model

```{r, message=FALSE,warning=FALSE}
plot(LM_model$finalModel)

```





### Linear Regression After Clustering

We wanted to test if we could improve the accuracy of the linear regression by first clustering the data using K means and then create a linear regression model for each cluster. Our assumption is that there is different subsets of the housing market and by clustering we will identify them and create specfic models for them.

####Data Normalization

Since the K-means algorithm using distance to find the nearest point we need to ensure all the variables are on the same range.  We do this by normalizing the data.  We also need to remove our Target variable SALE.PRICE from the cluster data.


```{r, message=FALSE,warning=FALSE}
#exclude Date column for clustering

is.num <- sapply(data_model, is.numeric)


df <- data.frame((data_model[,is.num]),encod_data)


#remove Sale.Price

df<- df[,!colnames(df)%in%c('SALE.PRICE')]

normalize = function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

df = as.data.frame(lapply(df, normalize))
```

####Finding Optimal Number of Clusters (K)

Before we run the K Mean algorithm we need to find the optimal K clusters. To find the optimal number of clusters we used the Elbow method. The elbow method finds the optimal K value by running mulptiple K-Means for K from 1 to N and calculateing the Sum of Squared errors (SSE).  We then plot each SSE for each K, we then chose an optimal value of K such that it has a low SSE but further increase of K would make little improvement to the SSE. That K value would be the elbow point of the plot.

```{r, message=FALSE,warning=FALSE}
#Find Optimal K using Elbow method

wssplot <- function(data, nc=15, seed=1234){
  wss <- (nrow(data)-1)*sum(apply(data,2,var))
  for (i in 2:nc){
    set.seed(seed)
    wss[i] <- sum(kmeans(data, centers=i, nstart=5)$withinss)}
  plot(1:nc, wss, type="b", xlab="Number of Clusters",
       ylab="Within groups sum of squares")
  }

wssplot(df, nc=10) 

```

Based on the Elbow method, we found the optimal number to be 6. Beyond 6, we see gradually decreasing improvements on the SSE.

#### Building K-Means Model 

We are now able to create the K means model, we are creating 4 clusters with the 4 corresponding centroids being randomly chosen 15 times.

```{r, message=FALSE,warning=FALSE}

set.seed(9897665)

kmeans.result <- kmeans(df, centers=6, nstart=15, iter.max = 100)

```

#### Building LM model for each cluster

We now build a linear regression model for each Cluster.

```{r, message=FALSE,warning=FALSE}

data_cluster <- data.frame(data_model,cluster=as.factor(kmeans.result$cluster))
#create blank dataframe to hold all clsuters
lmmodels_output <- data.frame(matrix(ncol = 5, nrow = 0))
x <- c("ClusterNumber", "MinMAxAccuracy", "MAPE","Within10","NumberofRecords")
colnames(lmmodels_output) <- x
for (i in levels(data_cluster$cluster)){
  train_set_c <- createDataPartition(data_cluster$BOROUGH[data_cluster$cluster==i], p = 0.8, list = FALSE)
  #Create train and test data
  train_data_c <- data_model[train_set_c,]
  test_data_C <- data_model[-train_set_c,]
  lmmodels <- train(SALE.PRICE~BLOCK+LOT+LAND.SQUARE.FEET+GROSS.SQUARE.FEET+YEAR.BUILT+BOROUGH+NEIGHBORHOOD+BUILDING.CLASS.CATEGORY +ZIP.CODE+data_out.SALE.DATE , data=train_data_c, method="lm",tuneLength = 50,
metric = "RMSE",
trControl = fitControl)
  LM_model.pred <- predict(LM_model, test_data_C)

  actuals_preds <- data.frame(cbind(actuals=test_data_C$SALE.PRICE, predicteds=LM_model.pred ))
  actuals_preds <- sapply(actuals_preds, function(x) 10^x)
  # Check Accuracy of Model 
  min_max_accuracy <- mean (apply(actuals_preds, 1, min) / apply(actuals_preds, 1, max))
  mape <- mean(abs((actuals_preds[,"predicteds"]  - actuals_preds[,"actuals"]))/actuals_preds[,"actuals"])
  Within10 <- ifelse(abs((actuals_preds[,"predicteds"] - actuals_preds[,"actuals"]))/actuals_preds[,"actuals"]>0.1,0,1)
  
  current_output <- data.frame(ClusterNumber = i,MinMAxAccuracy = min_max_accuracy,MAPE = mape,Within10 = mean(Within10) ,NumberofRecords = NROW(train_data_c))
  
  lmmodels_output <- rbind(lmmodels_output,current_output)
  
  
}

#Find Weighting Scores


lmmodels_output$WeightedWithin10 <- lmmodels_output$Within10 * lmmodels_output$NumberofRecords

#Weight Within10 Accuracy
Clustered_LM_Accuracy <- sum(lmmodels_output$WeightedWithin10) / sum(lmmodels_output$NumberofRecords)
lmmodels_output
basic_LM_Accuracy
Clustered_LM_Accuracy
round((Clustered_LM_Accuracy / basic_LM_Accuracy)  - 1,4)*100


```

We can see that running the linear regression model on the individual clusters increased the accuracy by `r round((Clustered_LM_Accuracy / basic_LM_Accuracy)  - 1,4)*100` %.

```{r, message=FALSE,warning=FALSE}
# # model <- "xgbTree"
# 
# 
#   
# 
# #########################################################################
# 
# xgbGrid <- expand.grid(nrounds = c(1, 10),
#                        max_depth = c(1, 4),
#                        eta = c(.1, .4),
#                        gamma = 0,
#                        colsample_bytree = .7,
#                        min_child_weight = 1,
#                        subsample = c(.8, 1))
# set.seed(2)
# cctrl1 <- trainControl(method = "cv", number = 3, returnResamp = "all",
#                        classProbs = TRUE, 
#                        summaryFunction = twoClassSummary)
# 
# test_class_cv_form <- train(SALE.PRICE~. , data=train_data, 
#                             method = "xgbTree", 
#                        
#                             metric = "MAE")
# 
# 
# 
# XG_model.pred <- predict(test_class_cv_form, test_data)
# 
# XG_actuals_preds <- data.frame(cbind(actuals=test_data$SALE.PRICE, predicteds=XG_model.pred )) 
# # Check Accuracy of Model 
# XG_min_max_accuracy <- mean (apply(XG_actuals_preds, 1, min) / apply(XG_actuals_preds, 1, max))
# XG_min_max_accuracy
# 
# XG_mape <- mean(abs((XG_actuals_preds$predicteds - XG_actuals_preds$actuals))/XG_actuals_preds$actuals) 
# XG_mape
# 
# #Within 10%
# XG_Within10 <- ifelse(abs((XG_actuals_preds$predicteds - XG_actuals_preds$actuals))/XG_actuals_preds$actuals>0.1,0,1)
# 
# XG_basic_LM_Accuracy <- mean(Within10)
# 
# XG_basic_LM_Accuracy



```
