---
title: "Property Value Prediction Using Clustering and Linear Regression - Automated Valuation Model (AVM)"
author: "Tyler Blakeley, Benjamin Kan, Mohammad Islam, Avijeet Singh"
date: "November 16 2018"
output:
  html_document:
    fig_height: 4.5
    fig_width: 7
    highlight: tango
    number_sections: yes
    theme: readable
    toc: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```

# Business Understanding

We are the owners of a start-up company based in New York City who specializes in appraisal management and national real estate information, servicing major banks, lenders, mortgage insurers, credit unions, and independent mortgage brokers. Currently, most of the real estate transactions including mortgage loans and commercial property acquisitions rely on appraisers to estimate the property values. However, this could take at least days to complete, which the mortgage lenders and brokers think takes too long given the hyper competitive nature of the real estate industry. Moreover, the property valuation process is at times manual without comprehensive substantiation of how the appraisers come up with the values. Therefore, we reckon that the real estate appraisal industry is ripe for disruption. Our product is a property value prediction model which can be used by mortgage lenders and brokers to value the properties within minutes.  It is a national solution built using highly advanced machine learning algorithms, which can take into account extensive and diverse data sets, learn from the data in their environment, as well as incorporate new and different situations, as the market evolves.           

# Data Understanding

## Data Source and Collection

We sourced the NYC property sales data from Kaggle (https://www.kaggle.com/new-york-city/nyc-property-sales/home). This dataset contains the properties sold in NYC over a 12-month period from September 2016 to September 2017. The dataset contains 22 attributes including the precise property locations (Borough, Neighborhood, Block, Zip Code and Address), building class, property types (Residential or Commercial), property size and tax class. The dataset consists of about 84.5K of property sale transactions.

## Data Description

In the dataset, there are 22 variables. A detailed description of these variables can be found at https://www1.nyc.gov/assets/finance/downloads/pdf/07pdf/glossary_rsf071607.pdf

Variable Name                   | Description
--------------------------------|---------------------------------------------------------------------------
 #                              | ID of the property
BOROUGH                         | The name of the borough in which the property is located                    
NEIGHBORHOOD                    | Neighborhood name in the course of valuing properties
BUILDING CLASS CATEGORY         | Building class descriptions
TAX CLASS                       | 1: Residential property up to three units; 2: All other property that is                                                                   | primarily residential (Co-ops and condos) 3: Property with equipment owned                                                                 | by a gas, telephone or electric company; 4: All other properties not                                                                       | included  
BLOCK                           | Sub-division of the borough on which the real properties are located
LOT                             | Subdivision of a block and represents the property unique location     
EASEMENT                        | A right which allows an entity to make limited use of another's real                                                                       | property
BUILDING CLASS AT PRESENT       | Building Code. Details can be found at
                                | https://www1.nyc.gov/assets/finance/jump/hlpbldgcode.html
ADDRESS                         | Street address of the property as listed on the Sales File. Coop sales                                                                     | include the apartment number
ZIP CODE                        | The property's postal code
RESIDENTIAL UNITS               | The number of residential units at the listed property
COMMERCIAL UNITS                | The number of commercial units at the listed property
LAND SQUARE FEET                | The land area of the property listed in square feet
GROSS SQUARE FEET               | The total area of all the floors of a building as measured from the exterior                                                               | surfaces of the outside walls of the building
YEAR BUILT                      | Year the structure on the property was built
BUILDING CLASS AT TIME OF SALE  | The Building Classification is used to describe a propertyâ€™s constructive                                                                  | use. The first position of the Building Class is a letter that is used to                                                                  | describe a general class of properties
SALES PRICE                     | Price paid for the property
SALE DATE                       | Date of property sold

Note: $0 Sales Price: A $0 sale indicates that there was a transfer of ownership without a cash consideration. There can be a number of reasons for a $0 sale including transfers of ownership from parents to children. 

## Data Quality Checks and Prelimiary Data Cleaning

### Load Packages

```{r, message=FALSE,warning=FALSE}
#import packages;
library(dplyr)
library(reshape2)
library(ggplot2)
library(Hmisc)
library(corrplot)
library(mice)
library(VIM)
library(pROC)
library(caret)
library(corrgram)
library(GGally)
library(ggthemes) 
library(DMwR)
library(gridExtra)
library(rattle)
library(readxl)
library(cluster)
library(sqldf)
library(knitr)
library(xgboost)
library(outliers)
```

### Load Datasets

Now that the packages are loaded, we can load in the dataset. 

```{r, message=FALSE,warning=FALSE}
data <- read.csv(file.choose(), header = TRUE, na.strings = c("NA","","#NA"," -  "))
```

Now let us see how our dataset looks like:

```{r, message=FALSE,warning=FALSE}
str(data)
```

From the above we can see that first column has only the observation numbers and is not necessary for our data exploration and modeling proesses so we will remove it.

```{r, message=FALSE,warning=FALSE}
data <- as_data_frame(data[,-1])
```

### Residential Data Selection

Based on our business case, we only want to create a model to predit the prices of the residentential homes.  We will then remove any sales data that are not residenttial homes based on the BUILDING.CLASS.CATEGORY. 


```{r, message=FALSE,warning=FALSE}

#Remove any home with facilities in its class
data <- data[!data$BUILDING.CLASS.CATEGORY %like% 'FACILITIES',]

#Remove any commerical
data <- data[!data$BUILDING.CLASS.CATEGORY %like% 'COMMERCIAL',]

#Remove office
data <- data[!data$BUILDING.CLASS.CATEGORY %like% 'OFFICE',]

#Remove HOTEL
data <- data[!data$BUILDING.CLASS.CATEGORY %like% 'HOTEL',]

#Remove Vacant Land
data <- data[!data$BUILDING.CLASS.CATEGORY %like% 'VACANT LAND',]

#Remove Other
data <- data[!data$BUILDING.CLASS.CATEGORY %like% 'OTHER',]

#Remove PARK
data <- data[!data$BUILDING.CLASS.CATEGORY %like% 'PARK',]

#Remove WAREHOUSE
data <- data[!data$BUILDING.CLASS.CATEGORY %like% 'WAREHOUSE',]

#Remove STORE
data <- data[!data$BUILDING.CLASS.CATEGORY %like% 'STORE',]

#Remove FACTORY
data <- data[!data$BUILDING.CLASS.CATEGORY %like% 'FACTOR',]

#Remove ASYLUM
data <- data[!data$BUILDING.CLASS.CATEGORY %like% 'ASYLUM',]

#Remove RENTAL
data <- data[!data$BUILDING.CLASS.CATEGORY %like% 'RENTAL',]
```

### Remove Number Codes in the Building Class Category

As we examine the BUILDING.CLASS.CATEGORY more, we notice that the some categories start with number codes.We think that this information is redundant and could introduce unnecessary complexity. Therefore, we will set up a new column which will only give us the BUILDING CLASS CATEGORY without the number codes i.e "walkup aprartments","elevator apartments" etc.

```{r, message=FALSE,warning=FALSE}
data$BUILDING.CLASS.CATEGORY<-substring(data$BUILDING.CLASS.CATEGORY,3)
head(data$BUILDING.CLASS.CATEGORY)
```

### Consolidate Condos and Coops Building Class Categories

We would like reduce the number of factors under the condos and coops building types by grouping various types of condos and coops as "CONDOS" and "COOP" respectively.

```{r, message=FALSE,warning=FALSE}
#Combine CONDO and COOP
data$BUILDING.CLASS.CATEGORY <- as.character(data$BUILDING.CLASS.CATEGORY)
data$BUILDING.CLASS.CATEGORY <- ifelse(data$BUILDING.CLASS.CATEGORY %like% 'CONDO', "CONDO",data$BUILDING.CLASS.CATEGORY)
data$BUILDING.CLASS.CATEGORY <- ifelse(data$BUILDING.CLASS.CATEGORY %like% 'COOP', "COOP",data$BUILDING.CLASS.CATEGORY)
data$BUILDING.CLASS.CATEGORY <- as.factor(data$BUILDING.CLASS.CATEGORY)
```

### Borough

Currently, the Borough column is numeric to represent the five boroughs in New York City:  Manhattan (1), Bronx (2), Brooklyn (3), Queens (4), and Staten Island (5). We will name the boroughs with proper names to provide more clarity and make our lives easier for the modeling work. 

```{r, message=FALSE,warning=FALSE}

data$BOROUGH <- ifelse(data$BOROUGH==1,'Manhattan',data$BOROUGH)
data$BOROUGH <- ifelse(data$BOROUGH==2,'Bronx',data$BOROUGH)
data$BOROUGH <- ifelse(data$BOROUGH==3,'Brooklyn',data$BOROUGH)
data$BOROUGH <- ifelse(data$BOROUGH==4,'Queens',data$BOROUGH)
data$BOROUGH <- ifelse(data$BOROUGH==5,'Staten Island',data$BOROUGH)

```

### EASE-MENT

We will remove the column "EASE-MENT" as it has null value throughout:

```{r, message=FALSE,warning=FALSE}
data$EASE.MENT<-NULL
```

### Add the Building Age Column

Now let us introduce a new column "Building Age" in years as it is much easier to understand.

```{r, message=FALSE,warning=FALSE}
data$BUILDING.AGE<- 2018 - data$YEAR.BUILT
```

### Check for Duplicate Data and Remove Those

Let us finally check for duplicates and remove if there are any:

```{r, message=FALSE,warning=FALSE}
data %>% filter(duplicated(data) == TRUE) %>% nrow()
```
From here we can see that there are 464 duplicated data elements in which we will remove them.

Removing duplicates:

```{r, message=FALSE,warning=FALSE}
data <- unique(data)
```

### Data Type conversions

The original dataset has columns with types that are not suitable for analysis so we will convert them to suitable types.

* The Boroughs, Building Class Category, Zipcode, Tax class are converted to factors.
* Address, Apartment Number converted to characters.
* Land Square Feet, Gross Square Feet, Year are converted to  numeric.
* Sale Date is converted to the date format.

```{r, message=FALSE,warning=FALSE}
fac<-c(1,3,10,17)
data[fac]<-lapply(data[fac],factor)
chr<-c(8,9)
data[chr]<-lapply(data[chr],as.character)
num<-c(14,15)
data[num]<-lapply(data[num],as.numeric)
dt<-c(20)
data[dt]<-lapply(data[dt],as.Date)

```

Let's now look at the data structure once again

```{r, message=FALSE,warning=FALSE}
str(data)
```

We are now at the stage where we can perform some data exploration. We will further fine tune our dataset as we go along.

## Data Exploration and Outlier Analysis

### Sale Price

First let us see the summary of sale price and then if the data is skewed by using the quantile function and box plot.

```{r, message=FALSE,warning=FALSE}
summary(data$SALE.PRICE)
#quantile(data$SALE.PRICE, probs = seq(from = 0, to = 1, by = .1))

ggplot(data, aes(,data$SALE.PRICE)) + geom_boxplot(fill = "white")+  scale_y_continuous("SALE PRICE")+scale_x_continuous("")+labs(title="Boxplot for Sale Prices")

```

As we can see, there are zero sale prices and substantial number of properties with very low sale prices which might not be correct. We will remove the outliers using the algorithms used in box and whisker plots.


```{r, message=FALSE,warning=FALSE}

#Remove sale prices with 0s and NAs, and with very low and high values (outliers)

data <- data[data$SALE.PRICE != 0,]
data <- data[!is.na(data$SALE.PRICE),]

data_rm_outliers <- data;

remove_outliers <- function(x,lbound) {
  qnt <- quantile(x, probs=c(.25, .75), na.rm = TRUE)
  H <- 1.5 * IQR(x, na.rm = TRUE)
  y=x
  y[x < (qnt[1] - H/lbound)] = -999999
  y[x > (qnt[2] + H)] = -999999
  y
  
}

data$SALE.PRICE <- remove_outliers(data_rm_outliers$SALE.PRICE,4)

data <- data[data$SALE.PRICE!=-999999,]

```

We have removed the missing and outlier SALE.PRICE data.  When we were removing outliers, we noticed that it was not removing any small values of SALE.PRICE, this was due to the fact that the minimum whisker was going negative and there is no sale price that is below zero.  We made a small change where we divide the inter quartile range by 4 for the minimum whisker to remove some small values of sale prices.

We have reduced our dataset to 46153 oberservations.

### Data Quality and Outlier Analysis for Gross.SQUARE.FEET, LAND.SQUARE.FEET and YEAR.BUILT

#### LAND.SQUARE.FEET

We noticed there are zero values for LAND.SQUARE.FEET, we want to investigate if this is an error or if there are some types of properties that have zero lot sizes. Specifically, we want to look at what is the LAND.SQUARE.FEET for condos.

First we will see how the data looks like:

```{r, message=FALSE,warning=FALSE}
summary(data$LAND.SQUARE.FEET)
```

Let us check for outliers

```{r, message=FALSE,warning=FALSE}
outlier(data$LAND.SQUARE.FEET)
```

Let us try to find out how the sale price is per land sq. feet.

First we will check for NA's in the Land square feet attribute:

```{r, message=FALSE,warning=FALSE}
sum(is.na(data$LAND.SQUARE.FEET))
```

We can see that there are `r sum(is.na(data$LAND.SQUARE.FEET))` instances with NAs and we will remove these for the time being to find the relationship between the square footage and the sale price. For this purpose only we will create a new data frame "nyc_data_sqfeet" and we will plot the graphs using this dataframe.

```{r, message=FALSE,warning=FALSE}
nyc_data_sqfeet <- as_data_frame(data[!is.na(data$LAND.SQUARE.FEET),])
```

Now that we have created the new data frame let us check for the relation between the square footage and the sale price.

```{r, message=FALSE,warning=FALSE}
ggplot(data = nyc_data_sqfeet, aes(x = log(nyc_data_sqfeet$LAND.SQUARE.FEET), y = log(nyc_data_sqfeet$SALE.PRICE), color = `BOROUGH`)) +
  geom_jitter() +
  geom_smooth(method = "lm") +
  theme(legend.position = "bottom") +
   ggtitle("Price Vs Land Square Footage in NYC", 
          subtitle = "Distribution of Sale Price vs Land Square feet Borough-wise") +
  scale_y_continuous("Property Sale Price", labels = scales::dollar) +
  scale_x_continuous("Land Square Footage", labels = scales::comma) +
   facet_wrap(~ BOROUGH) 
```
 
We can see that the sale price increases with the square footage across all boroughs except Manhattan. It could be because the distribution of the square footage of properties in Manhattan is not as dispersed so the log of the values is even more concentrated.


```{r, message=FALSE,warning=FALSE}
#Investigate Land square feet with zero values

sqldf('SELECT [BUILDING.CLASS.CATEGORY],COUNT(*) FROM data where [LAND.SQUARE.FEET] = 0 group by [BUILDING.CLASS.CATEGORY]')


```

We can see that the only type of houses with 0 LAND.SQUARE.FEET are condos and coops and this does not seem to be an error and requires no imputation.


#### GROSS.SQUARE.FEET

For GROSS.SQUARE.FEET we replaced any 0 values with NA, our assumaption is that no house can have 0 living area.

First we will see how the data looks like:

```{r, message=FALSE,warning=FALSE}
summary(data$GROSS.SQUARE.FEET)
```

Let us check for outliers

```{r, message=FALSE,warning=FALSE}
outlier(data$GROSS.SQUARE.FEET)
```

Let us try to find out how the sale price is per gros sq. feet.
First we will check for NA's in the gross square feet attribute:

```{r, message=FALSE,warning=FALSE}
sum(is.na(data$GROSS.SQUARE.FEET))
```

We can see that there are `r sum(is.na(data$GROSS.SQUARE.FEET))`  instances with NA as value and we will remove these for the time being in order to find the relationship between the square footage and the sale price. For this purpose only we will create a new data frame "nyc_data_grosssqfeet" and we will plot the graphs using this dataframe.

```{r, message=FALSE,warning=FALSE}
nyc_data_grosssqfeet <- as_data_frame(data[!is.na(data$GROSS.SQUARE.FEET),])
```

Now that we have created the new data frame let us check for the relation between the square footage and the sale price.

```{r, message=FALSE,warning=FALSE}
ggplot(data = nyc_data_grosssqfeet, aes(x = log(nyc_data_grosssqfeet$GROSS.SQUARE.FEET), y = log(nyc_data_grosssqfeet$SALE.PRICE), color = `BOROUGH`)) +
  geom_jitter() +
  geom_smooth(method = "lm") +
  facet_wrap(~ BOROUGH) +
  ggtitle("Price Vs Gross Square Footage in NYC",
          subtitle = "Distribution of Sale Price vs Gross Square feet Borough-wise") +
  scale_y_continuous("Property Sale Price", labels = scales::dollar) +
  scale_x_continuous("Gross Square Footage", labels = scales::comma) +
  theme(legend.position = "bottom")
```

We can see a similar trend as we saw with the land square footage. The sale price increases with the gross square footage. The ones for Mahattan is interesting. This seems to suggest that the square footage has a weak relationship with the sale price. This could be because Manhattan has a significant location premium being factored into the price, even more so than the square footage.

For the sold properties with gross square feet of zero, we will replace them with NAs as we will impute the data later on.


```{r, message=FALSE,warning=FALSE}

data$GROSS.SQUARE.FEET[data$GROSS.SQUARE.FEET == 0] <- NA

```

#### YEAR.BUILT

For YEAR.BUILT, we replace any house with the year built before 1776 to NA's.

```{r, message=FALSE,warning=FALSE}

data$YEAR.BUILT[data$YEAR.BUILT <= 1776] <- NA

```


### Boroughs

Let us see which borough recorded the the most property sales and which borough is the most expensive by avg. sale price.
```{r, message=FALSE,warning=FALSE}
p1 <- ggplot(data = data, aes(x = `BOROUGH`)) +
  geom_bar() +
  ggtitle("In-Demand Borough in NYC") +
  scale_y_continuous("# of Property Sales", labels = scales::comma) +
  scale_x_discrete("Borough")

p2 <- ggplot(data = data, aes(x = `BOROUGH`, y = mean(`SALE.PRICE`))) +
  geom_bar(stat = "identity") +
  ggtitle("Most Expensive Borough in NYC", subtitle = "Avg. Property Sale Price in NYC") +
  scale_y_continuous("Avg Sale Price", labels = scales::dollar) +
  scale_x_discrete("Borough")

grid.arrange(p1, p2, ncol = 1)
```

The plots show that Queens had the most number of property sales followed by Brooklyn. The highest average sale price is also in Queens followed by Brooklyn. This is surprising as we thought Manhattan would have the highest average sale price. So we suspect that there are some extremely expensive properties sold in Queens.

### Neigborhoods

Now that we have explored the sale price according to the boroughs, let us check for the most expensive neighborhoods.

We will only take the top ten neigborhoods as it will be difficult to plot all the neighborhoods.

```{r, message=FALSE,warning=FALSE}
dfnyc1<-as.data.frame(table(data$BOROUGH, data$NEIGHBORHOOD))
names(dfnyc1) <- c('BOROUGH','NEIGHBORHOOD', 'Freq')
dfnyc1 <- dfnyc1 %>% arrange(desc(Freq)) %>% head(10)
p3<-ggplot(data = dfnyc1,aes(x=dfnyc1$NEIGHBORHOOD,y=dfnyc1$Freq,fill=dfnyc1$BOROUGH))+geom_bar(stat = "identity")+scale_y_continuous("Number of sales")+scale_x_discrete("Neighborhood")+coord_flip()

dfnyc2<- 
  data %>% group_by(BOROUGH, NEIGHBORHOOD) %>% 
  summarise(MeanSP = mean(`SALE.PRICE`)) %>% 
  arrange(desc(MeanSP)) %>% head(10)

p4 <- ggplot(data = dfnyc2, aes(x = dfnyc2$NEIGHBORHOOD, y = dfnyc2$MeanSP, fill = dfnyc2$BOROUGH)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  ggtitle("Most Expensive Neighborhoods in NYC", 
          subtitle = "Top Neighborhoods by Avg Price") +
  scale_y_continuous("Avg Sale Price", labels = scales::dollar) +
  scale_x_discrete("Neighborhood") 

grid.arrange(p3,p4,ncol=1)
```

From the above graphs we can see that the most in-demand neighborhood is in Queens and the most expensive neighborhood by average price is in Little Italy in Manhattan. 

### Buildings

Let us explore the top ten most in-demand building class and the most expensive building class and to what borough they belong to.

```{r, message=FALSE,warning=FALSE}
dfnyc3<-as.data.frame(table(data$BOROUGH, data$BUILDING.CLASS.CATEGORY))
names(dfnyc3) <- c('BOROUGH','BUILDING CLASS CATEGORY', 'Freq')
dfnyc3 <- dfnyc3 %>% arrange(desc(Freq)) %>% head(10)
ggplot(data = dfnyc3,aes(x=dfnyc3$BOROUGH,y=dfnyc3$Freq,fill=dfnyc3$`BUILDING CLASS CATEGORY`))+geom_bar(stat = "identity",position = "dodge")+scale_y_continuous("Number of sales")+scale_x_discrete("Borough")


```

From the above diagram, it is clear that the one family dwellings are very popular in Queens and Staten Island. Condos are particularly in demand in Manhattan and in Brooklyn. 

Now lets take a look at the most expensive building class.

```{r, message=FALSE,warning=FALSE}
dfnyc4<- 
  data %>% group_by(BOROUGH,BUILDING.CLASS.CATEGORY ) %>% 
  summarise(MeanSP = mean(`SALE.PRICE`)) %>% 
  arrange(desc(MeanSP)) %>% head(10)

ggplot(data = dfnyc4, aes(x = dfnyc4$BUILDING.CLASS.CATEGORY, y = dfnyc4$MeanSP, fill = dfnyc4$BOROUGH)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  ggtitle("Most Expensive Buildings in NYC") +
  scale_y_continuous("Avg Sale Price", labels = scales::dollar) +
  scale_x_discrete("Building Type") + theme(legend.position = "bottom")

```
From the above graph we can see that the most expensive buildings are the three-family-dwellings in Manhattan and Brooklyn.

## Data Imputation

```{r, message=FALSE,warning=FALSE}

#Analyzed Missing values

aggr_plot = aggr(data, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(data), cex.axis=.7, gap=3, ylab=c("Histogram of missing data","Pattern"))


#remove easement columns all NA's and TOTAL.UNITS
data <- data[,!colnames(data) %in% c('EASE.MENT','X','TOTAL.UNITS')]

summary(data)

borough_list <- c('Manhattan','Queens','Bronx','Brooklyn','Staten Island')

for (i in borough_list) {
Gross <- median(data$GROSS.SQUARE.FEET[data$BOROUGH==i & !is.na(data$GROSS.SQUARE.FEET)])
data$GROSS.SQUARE.FEET[data$BOROUGH==i & is.na(data$GROSS.SQUARE.FEET)] <- Gross

Land <- median(data$LAND.SQUARE.FEET[data$BOROUGH==i & !is.na(data$LAND.SQUARE.FEET)])
data$LAND.SQUARE.FEET[data$BOROUGH==i & is.na(data$LAND.SQUARE.FEET)] <- Land

Year_Built <- median(data$YEAR.BUILT[data$BOROUGH==i & !is.na(data$YEAR.BUILT)])
data$YEAR.BUILT[data$BOROUGH==i & is.na(data$YEAR.BUILT)] <- Year_Built
}
summary(data$GROSS.SQUARE.FEET)
summary(data$LAND.SQUARE.FEET)
summary(data$YEAR.BUILT)

```

We imputed the 3 columns that had some missing values: GROSS.SQUARE.FEET, LAND.SQUARE.FEET and YEAR.BUILT. We imputed the values by taking the median value for the variable for the borough it is in.



#### Remove Outliers for LAND.SQUARE.FEET and GROSS.SQUARE.FEET -->

Now that we have inputed the NA's value in LAND.SQUARE.FEET and GROSS.SQUARE.FEET we can remove the outliers for these variables.

```{r, message=FALSE,warning=FALSE} 
data_rm_outliers_gsq_feet <- data

summary(data$GROSS.SQUARE.FEET)

data$GROSS.SQUARE.FEET <- remove_outliers(data_rm_outliers_gsq_feet$GROSS.SQUARE.FEET,4) 


data <- data[data$GROSS.SQUARE.FEET!=-999999,]
             
summary(data$GROSS.SQUARE.FEET)



data_rm_outliers_lsq_feet <- data

summary(data$LAND.SQUARE.FEET)

data$LAND.SQUARE.FEET[data$LAND.SQUARE.FEET!=0] <-remove_outliers(data_rm_outliers_lsq_feet$LAND.SQUARE.FEET[data_rm_outliers_lsq_feet$LAND.SQUARE.FEET!=0],1)


data <- data[data$LAND.SQUARE.FEET!=-999999,]
             
summary(data$LAND.SQUARE.FEET)

``` 
We implemented the same outlier detection (BOX Plot method) we used for SALE.PRICE.  When we were removing outliers for LAND.SQUARE.FEET we ignored 0 values as 0 is for condo building and we dont want to remove these.

After removing all outliers dataset now has `r NROW(data) ` elements. -->


###Check Variable Distribution


####SALE.PRCIE

Want to Check if the distrbution of SALE.PRICE is normal, if not covert to normal through transformation

```{r, message=FALSE,warning=FALSE}


plot(density(data$SALE.PRICE))

#Use log transformation to change to normal distrubution

plot(density(sapply(data$SALE.PRICE, log10)))

#Update SALE.PRICE

data$SALE.PRICE <- sapply(data$SALE.PRICE, log10)
```
We can see intially the SALE.PRICE was skewed towards the lower values, we used a log transformation to create a more normal distribution.


#Modelling

Before we can run the model we need to ensure all our variables are numeric. For any categorical variables we need to choose a way to encode it.

##Encoding Categorical Variables

To Encode the categorical variables we chose to replace each level of each variable with the average SALE.PRICE.


```{r, message=FALSE,warning=FALSE}
encod_Categories <- function (x,y) {
  z = y 
  for (i in levels(x)) {
    encod_value <- median(y[x==i])
 
    z[x==i] <-  encod_value
    }
  z
  
}

#Find all factor variables to encod

is.fact <- cbind('BOROUGH','NEIGHBORHOOD','BUILDING.CLASS.CATEGORY','TAX.CLASS.AT.PRESENT','ZIP.CODE','BUILDING.CLASS.AT.TIME.OF.SALE')

encod_data <- as.data.frame(data[,colnames(data)%in%is.fact])



for (i in 1:NCOL(encod_data)){
  
  encod_data[,i] <- encod_Categories(encod_data[,i],data$SALE.PRICE)
  
}

#Confirm all variables are numeric
str(encod_data)


table(data$BUILDING.CLASS.CATEGORY,encod_data$BUILDING.CLASS.CATEGORY)


```

We can see that all the columns are now numeric. Each level has been replaced with the median sale price value for that variable.


##Create dataset for modelling

Now that we have all the categorical variables converted to numeric,we can create a modelling dataset that includes the SALE.DATE field, inital numeric columns and encoded categorical columns.

```{r, message=FALSE,warning=FALSE}

is.num <- sapply(data, is.numeric)


data_model <- (data.frame((data[,is.num]),encod_data,data$SALE.DATE))

#Remove columns 

data_model <- data_model[,!colnames(data_model)%in%c('YEAR.BUILT','TAX.CLASS.AT.PRESENT','RESIDENTIAL.UNITS','COMMERCIAL.UNITS','TAX.CLASS.AT.TIME.OF.SALE','BUILDING.CLASS.AT.TIME.OF.SALE')]



```

We also remove all columns that a typically homeowner would not know about there house, we need to do this as our business model requires the user to enter information about the house.


##Look at Correalation

```{r, message=FALSE,warning=FALSE}
cor_result=rcorr(as.matrix(data_model[,1:10]))


# ++++++++++++++++++++++++++++
# flattenCorrMatrix - makes it easier to read (in my opinion)
# ++++++++++++++++++++++++++++
flattenCorrMatrix <- function(cormat, pmat) {
  ut <- upper.tri(cormat)
  data.frame(
    row = rownames(cormat)[row(cormat)[ut]],
    column = rownames(cormat)[col(cormat)[ut]],
    cor  =(cormat)[ut],
    p = pmat[ut]
  )
}

#Simple method to flatten (if that how you want to look at it)
cor_result_flat = flattenCorrMatrix(cor_result$r, cor_result$P)
cor_result_flat[cor_result_flat$cor >0.9,]
cor_result_flat[abs(cor_result_flat$cor) <0.1&cor_result_flat$column=='SALE.PRICE',]


```

## Split into Training & Test Sets

We split the data into 80% training data and 20% testing data, the data is balanced on the Borough variable to ensure each data is representative of the initial data set.

```{r, message=FALSE,warning=FALSE}
#set.seed(456292)

#Create index to split data
train_set <- createDataPartition(data_model$BOROUGH, p = 0.8, list = FALSE)



#Create train and test data
train_data <- data_model[train_set,]
test_data <- data_model[-train_set,]


```

## Build Model 

We decided to implement a linear regresion model.  With linear regression the goal is to identify a line that best first the data.  The line of best fit is the one where the prediction error is the least.  Since the linear regression tries to reduce the prediction error between actual and predicted ,linear regression can become skewed by outliers in the data.  We tried to reduce this by pre processing the data and removing the outliers.

We will create a basic linear regression on our train_data set using a 10 fold cross validation to help reduce over fitting.

```{r, message=FALSE,warning=FALSE}
#reduce bias using cross validation
folds=10

fitControl <- trainControl(method="cv",number=folds)

#Implement Linear Regresion Model (lm)
LM_model <- train(SALE.PRICE~., data=train_data, method="lm",
metric = "MAE",
trControl = fitControl)


```

We chose to run the linear regression intially on all of our variables.  We will now investigate if there is some variables we can remove that are of low signifiance.

## Model Assessment

### Variable Significance

```{r, message=FALSE,warning=FALSE}

summary(LM_model)

importance    <- varImp(LM_model)

ggplot(importance)

print(LM_model)


```


The most important variables are BUILDING.CLASS.CATEGORY, NEIGHBOURHOOD, LAND.SQUARE.FEET and GROSS.SQUARE.FEET. This was generally in line with what are assumptions were and what you hear about the housing market.  It is more about the house and location then the size and age of the house.  We were a little surprised that age had little to no effect but this could be because age is sometimes hard to judge with a house that has gone through extensive renovations.

All of the variables have low p-values, we can conclude our model is statistically significant.

The model has a Rsquared of 0.46, this means that the only 46% of the SALE.PRICE variable variation is explained by our linear model.  This was a lower Rquared then we expected since all of our variables are shown to be significant. We will now look at the residuals plot to help determine if the linear model is good or not.

### Check Residual of Linear Model

```{r, message=FALSE,warning=FALSE}
plot(LM_model$finalModel)

```


With linear regression we make the assumption that the errors will be normal distributed, to test that we will look at the Normal Q-Q plot.  This plot compares the quantiles of our errors to the quantiles of the normal distribution.  If the errors of our model was exactly normal we would expect to see a straight line.  We can see from the graph the line is fairly straight but curves off near the bottom quantiles.

Next we want to look at the Standardize residuals charted against the predicted value.  We expect the residuals to be random and there should be no observed pattern.  From the 3rd Chart we can the residuals look pretty random and there does not appear to be any pattern.


### Accuracy of Linear regression model

Now that we confirmed that we are using signicant variables and  residuals are random.  We can look into the accuracy of the model on the test data.

```{r, message=FALSE,warning=FALSE}

# Predict Test results
LM_model.pred <- predict(LM_model, test_data)


actuals_preds <- data.frame(cbind(actuals=test_data$SALE.PRICE, predicteds=LM_model.pred )) 

actuals_preds <- sapply(actuals_preds, function(x) 10^x)
# Check Accuracy of Model 
min_max_accuracy <- mean (apply(actuals_preds, 1, min) / apply(actuals_preds, 1, max))
min_max_accuracy
mape <- mean(abs((actuals_preds[,"predicteds"]  - actuals_preds[,"actuals"]))/actuals_preds[,"actuals"])

mape

#Within 10%


Within10 <- ifelse(abs((actuals_preds[,"predicteds"] - actuals_preds[,"actuals"]))/actuals_preds[,"actuals"]>0.1,0,1)

basic_LM_Accuracy <- mean(Within10)

basic_LM_Accuracy


```

We measure the accuracy 3 ways.  The first way was to look at the min max accuracy.  This looks at the ratio of min(acutal, predicted) / max(actual,predicted), this ratio tells how close the actual and predicted where to eachother.  1 is it is exactly the predicted, 0.5 means on average the they  50% of each other. (EX: 50K Predicted, 100K actual, or 200K predicted, 100k actual).  We had a min max accuracy of `r round(min_max_accuracy,4)` %

The second way was was to look at the Mean Absolute % change, this is percentage change between the absolute difference between predicted and actual.  We had an Mean absolute % change of `r round(mape,4)`%.

The third way to lok at what percent of the predicted were within 10% of the actual value.  We had `r round(basic_LM_Accuracy,4)`%  of properties that were within 10% of the actual value.

### Linear Regression After Clustering

We wanted to test if we could improve the accuracy of the linear regression by first clustering the data using K means and then create a linear regression model for each cluster. Our assumption is that there is different subsets of the housing market and by clustering we will identify them and create specfic models for them.

#### Data Normalization

Since the K-means algorithm using distance to find the nearest point we need to ensure all the variables are on the same range.  We do this by normalizing the data.  We also need to remove our Target variable SALE.PRICE from the cluster data.


```{r, message=FALSE,warning=FALSE}
#exclude Date column for clustering

is.num <- sapply(data_model, is.numeric)


df <- data.frame((data_model[,is.num]),encod_data)


#remove Sale.Price

df<- df[,!colnames(df)%in%c('SALE.PRICE')]

normalize = function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

df = as.data.frame(lapply(df, normalize))
```

#### Finding Optimal Number of Clusters (K)

Before we run the K Mean algorithm we need to find the optimal K clusters. To find the optimal number of clusters we used the Elbow method. The elbow method finds the optimal K value by running mulptiple K-Means for K from 1 to N and calculateing the Sum of Squared errors (SSE).  We then plot each SSE for each K, we then chose an optimal value of K such that it has a low SSE but further increase of K would make little improvement to the SSE. That K value would be the elbow point of the plot.

```{r, message=FALSE,warning=FALSE}
#Find Optimal K using Elbow method

wssplot <- function(data, nc=15, seed=1234){
  wss <- (nrow(data)-1)*sum(apply(data,2,var))
  for (i in 2:nc){
    set.seed(seed)
    wss[i] <- sum(kmeans(data, centers=i, nstart=5)$withinss)}
  plot(1:nc, wss, type="b", xlab="Number of Clusters",
       ylab="Within groups sum of squares")
  }

wssplot(df, nc=10) 

```

Based on the Elbow method, we found the optimal number to be 4. Beyond 4, we see gradually decreasing improvements on the SSE.

#### Building K-Means Model 

We are now able to create the K means model, we are creating 4 clusters with the 4 corresponding centroids being randomly chosen 15 times.

```{r, message=FALSE,warning=FALSE}

set.seed(9897665)

kmeans.result <- kmeans(df, centers=4, nstart=15, iter.max = 100)

```

#### Visualize Model with PCA

We will use Principle Componet Analysis (PCA) to help us Visualize our clusters.  PCA works by reducing dimensoinality of the data. We want to reduce the dimensionality to 2d because it will allow us to graph the clusters. PCA creates new columns that are linearly uncorrelated variables, and sorts thems in descending where PC1 will account for the highest variability of the data, and PC2 will be seocnd most and so on.

```{r, message=FALSE,warning=FALSE}
df_pca <- prcomp(df)
df_out <- as.data.frame(df_pca$x)

p<-ggplot(df_out,aes(x=PC1,y=PC2,color = as.factor(kmeans.result$cluster ) ))
theme<-theme(panel.background = element_blank(),panel.border=element_rect(fill=NA),panel.grid.major = element_blank(),panel.grid.minor = element_blank(),strip.background=element_blank(),axis.text.x=element_text(colour="black"),axis.text.y=element_text(colour="black"),axis.ticks=element_line(colour="black"),plot.margin=unit(c(1,1,1,1),"line"))
percentage <- round(df_pca$sdev / sum(df_pca$sdev) * 100, 2)
percentage <- paste( colnames(df_out), "(", paste( as.character(percentage), "%", ")", sep="") )

p<-p+geom_point()+theme+xlab(percentage[1]) + ylab(percentage[2])

p

```

Looking at the visualized data we can see that the clusters are well generlized and there is almost no overlap. Cluster 3 and 4 are almost fully homogeneous from each other. Cluster 1 and 2 seems to be some small overlapping on the bottom right of cluster 1.


#### Building LM model for each cluster

We now build a linear regression model for each Cluster.

```{r, message=FALSE,warning=FALSE}

data_cluster <- data.frame(data_model,cluster=as.factor(kmeans.result$cluster))
#create blank dataframe to hold all clsuters
lmmodels_output <- data.frame(matrix(ncol = 5, nrow = 0))
x <- c("ClusterNumber", "MinMAxAccuracy", "MAPE","Within10","NumberofRecords")
colnames(lmmodels_output) <- x
for (i in levels(data_cluster$cluster)){
  train_set_c <- createDataPartition(data_cluster$BOROUGH[data_cluster$cluster==i], p = 0.8, list = FALSE)
  #Create train and test data
  train_data_c <- data_model[train_set_c,]
  test_data_C <- data_model[-train_set_c,]
  lmmodels <- train(SALE.PRICE~., data=train_data_c, method="lm",tuneLength = 50,
metric = "MAE",
trControl = fitControl)
  LM_model.pred <- predict(lmmodels, test_data_C)

  actuals_preds <- data.frame(cbind(actuals=test_data_C$SALE.PRICE, predicteds=LM_model.pred ))
  actuals_preds <- sapply(actuals_preds, function(x) 10^x)
  # Check Accuracy of Model 
  min_max_accuracy <- mean (apply(actuals_preds, 1, min) / apply(actuals_preds, 1, max))
  mape <- mean(abs((actuals_preds[,"predicteds"]  - actuals_preds[,"actuals"]))/actuals_preds[,"actuals"])
  Within10 <- ifelse(abs((actuals_preds[,"predicteds"] - actuals_preds[,"actuals"]))/actuals_preds[,"actuals"]>0.1,0,1)
  
  current_output <- data.frame(ClusterNumber = i,MinMAxAccuracy = min_max_accuracy,MAPE = mape,Within10 = mean(Within10) ,NumberofRecords = NROW(train_data_c))
  
  lmmodels_output <- rbind(lmmodels_output,current_output)
  
  
}

#Find Weighting Scores


lmmodels_output$WeightedWithin10 <- lmmodels_output$Within10 * lmmodels_output$NumberofRecords

lmmodels_output$WeightedMAPE<- lmmodels_output$MAPE * lmmodels_output$NumberofRecords

lmmodels_output$weightedMinMAxAccuracy<- lmmodels_output$MinMAxAccuracy * lmmodels_output$NumberofRecords

lmmodels_output
#Weight Within10 Accuracy
Clustered_LM_Accuracy <- sum(lmmodels_output$WeightedWithin10) / sum(lmmodels_output$NumberofRecords)

Clustered_LM_MAPE <- sum(lmmodels_output$WeightedMAPE) / sum(lmmodels_output$NumberofRecords)

Clustered_LM_MinMax <- sum(lmmodels_output$weightedMinMAxAccuracy) / sum(lmmodels_output$NumberofRecords)



Final_Table <- data.frame(Model = "Basic_LM",MinMAxAccuracy = round(min_max_accuracy,4),MAPE = round(mape,4),Within10 = round(basic_LM_Accuracy,4))
Final_Table  <- rbind(Final_Table,data.frame(Model = "Cluster_LM",MinMAxAccuracy = round(Clustered_LM_MinMax,4),MAPE = round(Clustered_LM_MAPE,4),Within10 = round(Clustered_LM_Accuracy,4)))

Final_Table  <- rbind(Final_Table,data.frame(Model = "%Difference",MinMAxAccuracy = round((Clustered_LM_MinMax / min_max_accuracy)  - 1,4)*100,MAPE = round((Clustered_LM_MAPE / mape)  - 1,4)*100,Within10 = round((Clustered_LM_Accuracy / basic_LM_Accuracy)  - 1,4)*100))



kable(Final_Table,align = 'c',caption = "Model Accuracy")

```


We have created a model for each cluster, for each model we create a train and test data set based on the cluster data and ratioed on the borough variable.  For each model we calculated the same accuracy metrics.  We then create a weighted score for each accuracy metric, we do this by using the size of the cluster as the weight and times it by the accuracy metric.

We compared each of the accuracy metric to the basic LM model.

We observed the Cluster model was far worse in every category.  One reason could be the k means model clustered all the unique and very different properties and linear regression has a hard time predicting.  We can see there was one set of clusters that was significantly worse then the other ones.


### Other Models

Would using a more resource intensive model improve the results significantly?


#### Gradient Boosted Model (GBM)

GBM stands for Gradient Boosted  model orboosted tree regression model.  GBM works by first creating a basic linear regression model,  then it calculates what examples had the largest residuals.  The boosting refers to using the records with largest residual (poor predicted values) and training a new model of those values, this process is repeated until a certain stopping creteria is met.

The Boosted tree regression model has 3 parameters: tree depth, number of trees and the learning rate.

We used cross validation to find the optimal parameters.


```{r, message=FALSE,warning=FALSE}
gbm_fitControl <- trainControl(## 10-fold CV
                           method = "repeatedcv",
                           number = 5,
                           ## repeated ten times
                           repeats = 5)


gbmGrid <-  expand.grid(interaction.depth = c(5,9),
                        n.trees = (5:10)*200,
                        shrinkage = 0.1,
                        n.minobsinnode = 20)


set.seed(825)
gbmFit1 <- train(SALE.PRICE ~ ., data = train_data,
                 method = "gbm",
                 trControl = gbm_fitControl,
                 ## This last option is actually one
                 ## for gbm() that passes through
                 verbose = FALSE,
                 tuneGrid = gbmGrid
                 )

gbmFit1
plot(gbmFit1)

XG_model.pred <- predict(gbmFit1, test_data)
#convert SALE.PRICE back to intial values


XG_actuals_preds <- data.frame(cbind(actuals=test_data$SALE.PRICE, predicteds=XG_model.pred ))
XG_actuals_preds <- sapply(XG_actuals_preds, function(x) 10^x)
# Check Accuracy of Model
XG_min_max_accuracy <- mean (apply(XG_actuals_preds, 1, min) / apply(XG_actuals_preds, 1, max))

XG_mape <- mean(abs((XG_actuals_preds[,"predicteds"]  - XG_actuals_preds[,"actuals"]))/XG_actuals_preds[,"actuals"])


XG_Within10 <- ifelse(abs((XG_actuals_preds[,"predicteds"] - XG_actuals_preds[,"actuals"]))/XG_actuals_preds[,"actuals"]>0.1,0,1)

XG_Within10_Accuracy <- mean(XG_Within10)


XG_Within10_Accuracy


Final_Table  <- rbind(Final_Table,data.frame(Model = "GBM",MinMAxAccuracy = round(XG_min_max_accuracy,4),MAPE = round(XG_mape,4),Within10 = round(XG_Within10_Accuracy,4)))

kable(Final_Table)

```

We can see the GBM is much better among all 3 accuracy metrics. 

#Next Steps

The accuracy is still well below most commercial grade Housing Estimators.  The largest one is the zestimate model created by zillow where the model achieved an accuracy of 71% of the properties within 10% of the actual sale prices.  Next step would be to continue to refine the model and try to increase the accuracy by gathering more data and attributes on the properties.  Some the of attributes we could look into collecting and spatial data on how close houses are amenties / highways / lakes / schools.  We could also see if we could get data that reflects the property conditions.

https://www.zillow.com/zestimate/#acc


#Deployment

With the current accuracy we would suggest not using this as the sole model to estimate / appraise properties.  In the current configuration, we would deploy it as a secondary check to help provide confirmation of the property values.  We could provide an estimated value on every property and in the case there is a dispute or a wide variance, we can use the estimate to help make our case how much the property is worth.

To help the clients better use the prediction model, we created a user interface where the user can input the zip code, neighborhood, property type, square footage and the property age and then the model will predict the estimated price. Note that the model used in the app is Gradient Boosted Model (GBM) which gives the best model accuracy among the three.



