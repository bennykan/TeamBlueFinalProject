---
title: "Property Value Prediction Using Clustering and Linear Regression - Automated Valuation Model (AVM)"
author: "Tyler Blakeley, Benjamin Kan, Mohammad Islam, Avijeet Singh"
date: "November 16 2018"
output:
  html_document:
    fig_height: 4.5
    fig_width: 7
    highlight: tango
    number_sections: yes
    theme: readable
    toc: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```

# Business Understanding

We are the owners of a start-up company based in New York City who specializes in appraisal management and national real estate information, servicing major banks, lenders, mortgage insurers, credit unions, and independent mortgage brokers. Currently, most of the real estate transactions including mortgage loans and commercial property acquisitions rely on appraisers to estimate the property values. However, this could take at least days to complete, which the mortgage lenders and brokers think takes too long given the hyper competitive nature of the real estate industry. Moreover, the property valuation process is at times manual without comprehensive substantiation of how the appraisers come up with the values. Therefore, we reckon that the real estate appraisal industry is ripe for disruption. Our product is a property value prediction model which can be used by mortgage lenders and brokers to value the properties within minutes.  It is a national solution built using highly advanced machine learning algorithms, which can take into account extensive and diverse data sets, learn from the data in their environment, as well as incorporate new and different situations, as the market evolves.           

# Data Understanding

## Data Source and Collection

We sourced the NYC property sales data from Kaggle (https://www.kaggle.com/new-york-city/nyc-property-sales/home). This dataset contains the properties sold in NYC over a 12-month period from September 2016 to September 2017. The dataset contains 22 attributes including the precise property locations (Borough, Neighborhood, Block, Zip Code and Address), building class, property types (Residential or Commercial), property size and tax class. The dataset consists of about 84.5K of property sale transactions.

## Data Description

In the dataset, there are 22 variables. A detailed description of these variables can be found at https://www1.nyc.gov/assets/finance/downloads/pdf/07pdf/glossary_rsf071607.pdf

Variable Name                   | Description
--------------------------------|---------------------------------------------------------------------------
 #                              | ID of the property
BOROUGH                         | The name of the borough in which the property is located                    
NEIGHBORHOOD                    | Neighborhood name in the course of valuing properties
BUILDING CLASS CATEGORY         | Building class descriptions
TAX CLASS                       | 1: Residential property up to three units; 2: All other property that is                                    | primarily residential (Co-ops and condos) 3: Property with equipment owned                                  | by a gas, telephone or electric company; 4: All other properties not                                        | included  
BLOCK                           | Sub-division of the borough on which the real properties are located
LOT                             | Subdivision of a block and represents the property unique location     
EASEMENT                        | A right which allows an entity to make limited use of another's real                                        | property
BUILDING CLASS AT PRESENT       | Building Code. Details can be found at
                                | https://www1.nyc.gov/assets/finance/jump/hlpbldgcode.html
ADDRESS                         | Street address of the property as listed on the Sales File. Coop sales                                                               | include the apartment number
ZIP CODE                        | The property's postal code
RESIDENTIAL UNITS               | The number of residential units at the listed property
COMMERCIAL UNITS                | The number of commercial units at the listed property
LAND SQUARE FEET                | The land area of the property listed in square feet
GROSS SQUARE FEET               | The total area of all the floors of a building as measured from the exterior                                 | surfaces of the outside walls of the building
YEAR BUILT                      | Year the structure on the property was built
BUILDING CLASS AT TIME OF SALE  | The Building Classification is used to describe a propertyâ€™s constructive                                   | use. The first position of the Building Class is a letter that is used to                                   | describe a general class of properties
SALES PRICE                     | Price paid for the property
SALE DATE                       | Date of property sold

Note: $0 Sales Price: A $0 sale indicates that there was a transfer of ownership without a cash consideration. There can be a number of reasons for a $0 sale including transfers of ownership from parents to children. 

## Data Exploration

### Load Packages

```{r, message=FALSE,warning=FALSE}
#import packages;
library(dplyr)
library(reshape2)
library(ggplot2)
library(Hmisc)
library(corrplot)
library(mice)
library(VIM)
library(pROC)
library(caret)
library(corrgram)
library(GGally)
library(ggthemes) 
library(DMwR)
library(gridExtra)
library(rattle)
library(readxl)
library(cluster)
library(sqldf)
library(knitr)
```

### Load Datasets

Now that the packages are loaded, we can load in the dataset. Note that we converted the dataset file from the Excel to csv format.


```{r, message=FALSE,warning=FALSE}
data=read.csv(file.choose(), header = TRUE, na.strings = c("NA","","#NA"," -  "))
```

Now let us see how our dataset looks like:

```{r, message=FALSE,warning=FALSE}
str(data)
```

From the above we can see that 1st column has only the observation numbers and is not necessary for our data exploration so i will remove it for the time being and make a new dataset.

```{r, message=FALSE,warning=FALSE}
nyc_data<-as_data_frame(data[,-1])
```

Now we will introduce a new column which will only give me the BUILDING CLASS CATEGORY i.e "walkup aprartments","elevator apartments" etc. and not the numbers in front of them as they are not needed.
First i will remove the first three characters and check if it worked.

```{r, message=FALSE,warning=FALSE}
nyc_data$BUILDING.CLASS.CATEGORY<-substring(nyc_data$BUILDING.CLASS.CATEGORY,3)
head(nyc_data$BUILDING.CLASS.CATEGORY)
```

Instead of the numbers in the BOROUGH column we will put in the names:

```{r, message=FALSE,warning=FALSE}

nyc_data$BOROUGH <- ifelse(nyc_data$BOROUGH==1,'Manhattan',nyc_data$BOROUGH)
nyc_data$BOROUGH <- ifelse(nyc_data$BOROUGH==2,'Bronx',nyc_data$BOROUGH)
nyc_data$BOROUGH <- ifelse(nyc_data$BOROUGH==3,'Brooklyn',nyc_data$BOROUGH)
nyc_data$BOROUGH <- ifelse(nyc_data$BOROUGH==4,'Queens',nyc_data$BOROUGH)
nyc_data$BOROUGH <- ifelse(nyc_data$BOROUGH==5,'Staten Island',nyc_data$BOROUGH)




data$BOROUGH <- ifelse(data$BOROUGH==1,'Manhattan',data$BOROUGH)
data$BOROUGH <- ifelse(data$BOROUGH==2,'Bronx',data$BOROUGH)
data$BOROUGH <- ifelse(data$BOROUGH==3,'Brooklyn',data$BOROUGH)
data$BOROUGH <- ifelse(data$BOROUGH==4,'Queens',data$BOROUGH)
data$BOROUGH <- ifelse(data$BOROUGH==5,'Staten Island',data$BOROUGH)


```

We will remove the column "EASE-MENT" as it has null value throughout:

```{r, message=FALSE,warning=FALSE}
nyc_data$EASE.MENT<-NULL
```

Now let us introduce a new column "Building Age" as it is much simpler to understand.

```{r, message=FALSE,warning=FALSE}
nyc_data$BUILDING.AGE<- 2018-nyc_data$YEAR.BUILT
```

From the glossary of terms we find out that there are only 4 types of tax class i.e 1,2,3,4 but in the dataset we observed that there are 11 different levels with the types further divided into 1a,1b,2a,2b etc. So we will remove the sub classes and just keep 4 levels of Tax as it will make our analysis easier.

```{r, message=FALSE,warning=FALSE}
levels(nyc_data$TAX.CLASS.AT.PRESENT)[levels(nyc_data$TAX.CLASS.AT.PRESENT=="1A")]<-"1"

                                      
str(nyc_data$TAX.CLASS.AT.PRESENT)
```



Let us finally check for duplicates and remove if there are any:

```{r, message=FALSE,warning=FALSE}
nyc_data %>% filter(duplicated(nyc_data) == TRUE) %>% nrow()
```


Removing duplicates:

```{r, message=FALSE,warning=FALSE}
nyc_data<-unique(nyc_data) 
```

Getting rid of NAs in the Sale price data as we need to take mean of the sale price in our data analysis.
```{r, message=FALSE,warning=FALSE}
nyc_data <- nyc_data[!is.na(nyc_data$SALE.PRICE),]
nyc_data <- nyc_data[nyc_data$SALE.PRICE != 0,]
```
###Data Type conversions
The original dataset has columns with types that are not suitable for analysis so we will convert them.
The Boroughs,building class category,zipcode,tax class at the time of sale  are converted to factors.
Address,apartment number converted to chr.
Land square feet,gross square feet,year to numeric.
Sale date to date.

```{r, message=FALSE,warning=FALSE}
fac<-c(1,3,10,17)
nyc_data[fac]<-lapply(nyc_data[fac],factor)
chr<-c(8,9)
nyc_data[chr]<-lapply(nyc_data[chr],as.character)
num<-c(14,15)
nyc_data[num]<-lapply(nyc_data[num],as.numeric)
dt<-c(20)
nyc_data[dt]<-lapply(nyc_data[dt],as.Date)
```

No let us begin our data exploration.

###Boroughs
Let us see which borough recorded the the most property sales and which borough is the most expensive by avg. sale price.
```{r, message=FALSE,warning=FALSE}
p1 <- ggplot(data = nyc_data, aes(x = `BOROUGH`)) +
  geom_bar() +
  ggtitle("In-Demand Borough in NYC") +
  scale_y_continuous("# of Property Sales", labels = scales::comma) +
  scale_x_discrete("Borough")

p2 <- ggplot(data = nyc_data, aes(x = `BOROUGH`, y = mean(`SALE.PRICE`))) +
  geom_bar(stat = "identity") +
  ggtitle("Most Expensive Borough in NYC", subtitle = "Avg. Property Sale Price in NYC") +
  scale_y_continuous("Avg Sale Price", labels = scales::dollar) +
  scale_x_discrete("Borough")

grid.arrange(p1, p2, ncol = 1)
```

The plots show that queens had the most number of property sales followed by brooklyn. The average sale price of a property in queens is the highest followed by brooklyn.

###Neigborhoods
Now that we have explored the sale price according to the boroughs let us check for the most expensive neighborhoods.
We will only take the top ten neigborhoods as it will be difficult to plot all the neighborhoods.
```{r, message=FALSE,warning=FALSE}
dfnyc1<-as.data.frame(table(nyc_data$BOROUGH, nyc_data$NEIGHBORHOOD))
names(dfnyc1) <- c('BOROUGH','NEIGHBORHOOD', 'Freq')
dfnyc1 <- dfnyc1 %>% arrange(desc(Freq)) %>% head(10)
p3<-ggplot(data = dfnyc1,aes(x=dfnyc1$NEIGHBORHOOD,y=dfnyc1$Freq,fill=dfnyc1$BOROUGH))+geom_bar(stat = "identity")+scale_y_continuous("Number of sales")+scale_x_discrete("Neighborhood")+coord_flip()

dfnyc2<- 
  nyc_data %>% group_by(BOROUGH, NEIGHBORHOOD) %>% 
  summarise(MeanSP = mean(`SALE.PRICE`)) %>% 
  arrange(desc(MeanSP)) %>% head(10)

p4 <- ggplot(data = dfnyc2, aes(x = dfnyc2$NEIGHBORHOOD, y = dfnyc2$MeanSP, fill = dfnyc2$BOROUGH)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  ggtitle("Most Expensive Neighborhoods in NYC", 
          subtitle = "Top Neighborhoods by Avg Price") +
  scale_y_continuous("Avg Sale Price", labels = scales::dollar) +
  scale_x_discrete("Neighborhood") 

grid.arrange(p3,p4,ncol=1)
```

From the above graphs we can see that the most in-demand neighborhood is in Queens and the most expensive neighborhood by avg. price is in Staten island.


###Buildings
Let us explore the top ten most in-demand building class and the most expensive building class and to what borough do they belong to.
```{r, message=FALSE,warning=FALSE}
dfnyc3<-as.data.frame(table(nyc_data$BOROUGH, nyc_data$BUILDING.CLASS.CATEGORY))
names(dfnyc3) <- c('BOROUGH','BUILDING CLASS CATEGORY', 'Freq')
dfnyc3 <- dfnyc3 %>% arrange(desc(Freq)) %>% head(10)
p5<-ggplot(data = dfnyc3,aes(x=dfnyc3$BOROUGH,y=dfnyc3$Freq,fill=dfnyc3$`BUILDING CLASS CATEGORY`))+geom_bar(stat = "identity",position = "dodge")+scale_y_continuous("Number of sales")+scale_x_discrete("Borough")
p5

```

From the above diagram it is clear that the one family dwellings are very popular in queens and staten island.
Condos are particulary in demand in manhattan and in Brooklyn the two family dwellings are more popular than the rest.
Now lets take a look at the most expensive building class.

```{r, message=FALSE,warning=FALSE}
dfnyc4<- 
  nyc_data %>% group_by(BOROUGH,BUILDING.CLASS.CATEGORY ) %>% 
  summarise(MeanSP = mean(`SALE.PRICE`)) %>% 
  arrange(desc(MeanSP)) %>% head(10)

p6 <- ggplot(data = dfnyc4, aes(x = dfnyc4$BUILDING.CLASS.CATEGORY, y = dfnyc4$MeanSP, fill = dfnyc4$BOROUGH)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  ggtitle("Most Expensive Buildings in NYC") +
  scale_y_continuous("Avg Sale Price", labels = scales::dollar) +
  scale_x_discrete("Building Type") + theme(legend.position = "bottom")
p6
```
From the above graph we can see that the most expensive buildings are the luxury hotels in Manhattan.


###Building Age
```{r, message=FALSE,warning=FALSE}

```
#Check current variable type
```{r, message=FALSE,warning=FALSE}
str(data)

#Change the Borough to Categorical

data$BOROUGH <- as.factor(data$BOROUGH)

#Change ZIP.CODE to categorical

data$ZIP.CODE <- as.factor(data$ZIP.CODE)

#Change Sale.DATE to date 

data$SALE.DATE <- as.Date(data$SALE.DATE)

str(data)

```

```{r, message=FALSE,warning=FALSE}

summary(data)



#Remove sale price of 0 and NA's
data <- data[data$SALE.PRICE != 0,]
data <- data[!is.na(data$SALE.PRICE),]

#Remove any house built before 1776 america
#data <- data[data$YEAR.BUILT>=1776,]



#Remove outliers for sale.Price

# sp_plot = ggplot(data, aes(data$SALE.PRICE)) + geom_histogram(binwidth = 10000,colour="black",fill="white")+  scale_x_continuous("Sale.Price")+scale_y_continuous("Observations Count")+labs(title = "Histogram")
# sp_plot_2 = ggplot(data, aes(,data$SALE.PRICE)) + geom_boxplot(fill = "white")+  scale_y_continuous("Sale.Price")+scale_x_continuous("")+labs(title="Boxplot")
# sp_plot_2
# grid.arrange(sp_plot,sp_plot_2,nrow =1,top="SALE.PRICE DISTRIBUTION AND OUTLIERS" )



data_rm_outliers = data;

remove_outliers <- function(x,lbound) {
  qnt <- quantile(x, probs=c(.25, .75), na.rm = TRUE)
  H <- 1.5 * IQR(x, na.rm = TRUE)
  y=x
  y[x < (qnt[1] - H/lbound)] = -999999
  y[x > (qnt[2] + H)] = -999999
  y
  
}


data$SALE.PRICE <- remove_outliers(data_rm_outliers$SALE.PRICE,4)

data_out <- data[data$SALE.PRICE!=-999999,]





```





```{r, message=FALSE,warning=FALSE}

data_out$GROSS.SQUARE.FEET[data_out$GROSS.SQUARE.FEET == 0] <- NA

data_out$YEAR.BUILT[data_out$YEAR.BUILT <= 1776] <- NA

summary(data_out)


```


```{r, message=FALSE,warning=FALSE}
#Investigate Land square feet

table(data[data$LAND.SQUARE.FEET == 0,c('LAND.SQUARE.FEET','BUILDING.CLASS.CATEGORY')])




```


```{r, message=FALSE,warning=FALSE}

#Analyzed Missing values

aggr_plot = aggr(data_out, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(data_out), cex.axis=.7, gap=3, ylab=c("Histogram of missing data","Pattern"))


#remove easement columns all NA's and TOTAL.UNITS
data_out <- data_out[,!colnames(data_out) %in% c('EASE.MENT','X','TOTAL.UNITS')]

summary(data_out)

imputed_values <- function(x,borough) {
  qnt <- quantile(x, probs=c(.25, .75), na.rm = TRUE)
  H <- 1.5 * IQR(x, na.rm = TRUE)
  y=x
  y[x < (qnt[1] - H/lbound)] = -999999
  y[x > (qnt[2] + H)] = -999999
  y
  
}



borough_list <- c('Manhattan','Queens','Bronx','Brooklyn','Staten Island')

for (i in borough_list) {
Gross <- median(data_out$GROSS.SQUARE.FEET[data_out$BOROUGH==i & !is.na(data_out$GROSS.SQUARE.FEET)])
data_out$GROSS.SQUARE.FEET[data_out$BOROUGH==i & is.na(data_out$GROSS.SQUARE.FEET)] <- Gross

Land <- median(data_out$LAND.SQUARE.FEET[data_out$BOROUGH==i & !is.na(data_out$LAND.SQUARE.FEET)])
data_out$LAND.SQUARE.FEET[data_out$BOROUGH==i & is.na(data_out$LAND.SQUARE.FEET)] <- Land

Year_Built <- median(data_out$YEAR.BUILT[data_out$BOROUGH==i & !is.na(data_out$YEAR.BUILT)])
data_out$YEAR.BUILT[data_out$BOROUGH==i & is.na(data_out$YEAR.BUILT)] <- Year_Built
}
summary(data_out$GROSS.SQUARE.FEET)
summary(data_out$LAND.SQUARE.FEET)
summary(data_out$YEAR.BUILT)

summary(data_out)



```

##Filter For Residential Data

```{r, message=FALSE,warning=FALSE}


#Remove any home with facilities in its class
data_out <- data_out[!data_out$BUILDING.CLASS.CATEGORY %like% 'FACILITIES',]

#Remove any commerical
data_out <- data_out[!data_out$BUILDING.CLASS.CATEGORY %like% 'COMMERCIAL',]

#Remove office
data_out <- data_out[!data_out$BUILDING.CLASS.CATEGORY %like% 'OFFICE',]


#Remove HOTEL
data_out <- data_out[!data_out$BUILDING.CLASS.CATEGORY %like% 'HOTEL',]

#Remove Vacant Land
data_out <- data_out[!data_out$BUILDING.CLASS.CATEGORY %like% 'VACANT LAND',]

#Remove Other
data_out <- data_out[!data_out$BUILDING.CLASS.CATEGORY %like% 'OTHER',]

#Remove PARK
data_out <- data_out[!data_out$BUILDING.CLASS.CATEGORY %like% 'PARK',]

#Remove WAREHOUSE
data_out <- data_out[!data_out$BUILDING.CLASS.CATEGORY %like% 'WAREHOUSE',]

#Remove STORE
data_out <- data_out[!data_out$BUILDING.CLASS.CATEGORY %like% 'STORE',]

#Remove FACTORY
data_out <- data_out[!data_out$BUILDING.CLASS.CATEGORY %like% 'FACTOR',]


#Remove ASYLUM
data_out <- data_out[!data_out$BUILDING.CLASS.CATEGORY %like% 'ASYLUM',]

#Remove RENTAL
data_out <- data_out[!data_out$BUILDING.CLASS.CATEGORY %like% 'RENTAL',]


data_out$BUILDING.CLASS.CATEGORY <- as.character(data_out$BUILDING.CLASS.CATEGORY)
data_out$BUILDING.CLASS.CATEGORY <- ifelse(data_out$BUILDING.CLASS.CATEGORY %like% 'CONDO', "CONDO",data_out$BUILDING.CLASS.CATEGORY)
data_out$BUILDING.CLASS.CATEGORY <- ifelse(data_out$BUILDING.CLASS.CATEGORY %like% 'COOP', "COOP",data_out$BUILDING.CLASS.CATEGORY)

data_out$BUILDING.CLASS.CATEGORY <- as.factor(data_out$BUILDING.CLASS.CATEGORY)

```


#Modelling


##Encoding Categorical Variables

To Encode the categorical variables we chose to replace each level of each variable with the average SALE.PRICE.


```{r, message=FALSE,warning=FALSE}
encod_Categories <- function (x,y) {
  z = y 
  for (i in levels(x)) {
    encod_value <- median(y[x==i])
    
    #print(NROW(y[x==i]))
    #print(encod_value)
    z[x==i] <-  encod_value
    #print(NROW(z[x==i]))
    #print(table(z[x==i] ))
    }
  z
  
}

#Find all factor variables to encod

is.fact <- cbind('BOROUGH','NEIGHBORHOOD','BUILDING.CLASS.CATEGORY','TAX.CLASS.AT.PRESENT','ZIP.CODE','BUILDING.CLASS.AT.TIME.OF.SALE')

encod_data <- data_out[,is.fact]
for (i in 1:NCOL(encod_data)){
  encod_data[,i] <- encod_Categories(encod_data[,i],data_out$SALE.PRICE)
  
}

#Confirm all variables are numeric
str(encod_data)

```



##Create dataset for modelling

Now that we have all the categorical variables converted to numeric,we can add in the date and other numeric columns

```{r, message=FALSE,warning=FALSE}

is.num <- sapply(data_out, is.numeric)


data_model <- (data.frame((data_out[,is.num]),encod_data,data_out$SALE.DATE))



```

##Look at Correalation

```{r, message=FALSE,warning=FALSE}
cor_result=rcorr(as.matrix(data_model[,1:15]))


# ++++++++++++++++++++++++++++
# flattenCorrMatrix - makes it easier to read (in my opinion)
# ++++++++++++++++++++++++++++
flattenCorrMatrix <- function(cormat, pmat) {
  ut <- upper.tri(cormat)
  data.frame(
    row = rownames(cormat)[row(cormat)[ut]],
    column = rownames(cormat)[col(cormat)[ut]],
    cor  =(cormat)[ut],
    p = pmat[ut]
  )
}

#Simple method to flatten (if that how you want to look at it)
cor_result_flat = flattenCorrMatrix(cor_result$r, cor_result$P)
cor_result_flat



```




## Split into Training & Test Sets

We split the data into 80% training data and 20% testing data, the data is balanced on the Borough variable to ensure each data is representative of the initial data set.

```{r, message=FALSE,warning=FALSE}
#set.seed(456292)

#Create index to split data
train_set <- createDataPartition(data_model$BOROUGH, p = 0.8, list = FALSE)



#Create train and test data
train_data <- data_model[train_set,]
test_data <- data_model[-train_set,]


```

## Build Model 

We decided to implement a linear regresion model.  With linear regression the goal is to identify a line that best first the data.  The line of best fit is the one which the prediction error is the least.

```{r, message=FALSE,warning=FALSE}
#Find Optimal Parameters for Decision Tree Model and reduce bias using cross validation
folds=10

fitControl <- trainControl(method="cv",number=folds)

#Implement Decision Tree model 
LM_model <- train(SALE.PRICE~., data=train_data, method="lm",tuneLength = 50,
metric = "RMSE",
trControl = fitControl)


```

We chose to run the linear regression intially on all of our variables.  We will now investigate if there is some variables we can remove that are of low signifiance.

## Assess Model



###variable Significance

```{r, message=FALSE,warning=FALSE}

summary(LM_model)

#Can see TAX.CLASS.AT.PRESENT has high P value , will remove from next model

LM_model <- train(SALE.PRICE~BLOCK+LOT+LAND.SQUARE.FEET+GROSS.SQUARE.FEET+YEAR.BUILT+BOROUGH+NEIGHBORHOOD+BUILDING.CLASS.CATEGORY +ZIP.CODE+data_out.SALE.DATE , data=train_data, method="lm",tuneLength = 50,
metric = "RMSE",
trControl = fitControl)

summary(LM_model$finalModel)




print(LM_model)



```
### Accuracy of LM

```{r, message=FALSE,warning=FALSE}

# Predict Test results
LM_model.pred <- predict(LM_model, test_data)

actuals_preds <- data.frame(cbind(actuals=test_data$SALE.PRICE, predicteds=LM_model.pred )) 
# Check Accuracy of Model 
min_max_accuracy <- mean (apply(actuals_preds, 1, min) / apply(actuals_preds, 1, max))
min_max_accuracy
mape <- mean(abs((actuals_preds$predicteds - actuals_preds$actuals))/actuals_preds$actuals) 
mape

#Within 10%
Within10 <- ifelse(abs((actuals_preds$predicteds - actuals_preds$actuals))/actuals_preds$actuals>0.1,0,1)

basic_LM_Accuracy <- mean(Within10)

basic_LM_Accuracy


```

The linear regression has an intial accuracy of `r ROUND(basic_LM_Accuracy,4) % ` within 10% of actual value.


####Check Residual of Linear Model

```{r, message=FALSE,warning=FALSE}
plot(LM_model$finalModel)

```

### Linear Regression After Clustering

We wanted to test if we could improve the accuracy of the linear regression by first clustering the data using K means and then create a linear regression model for each cluster.

####Data Normalization

Since the K-means algorithm using distance to find the nearest point we need to ensure all the variables are on the same range.  We do this by normalizing the data.

```{r, message=FALSE,warning=FALSE}
#exclude Date column for clustering

df <- data.frame((data_out[,is.num]),encod_data)

normalize = function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

df = as.data.frame(lapply(df, normalize))
```

####Finding Optimal Number of Clusters (K)

Before we run the K Mean algorithm we need to find the optimal K clusters. To find the optimal number of clusters we used the Elbow method. The elbow method finds the optimal K value by running mulptiple K-Means for K from 1 to N and calculateing the Sum of Squared errors (SSE).  We then plot each SSE for each K, we then chose an optimal value of K such that it has a low SSE but further increase of K would make little improvement to the SSE. That K value would be the elbow point of the plot.

```{r, message=FALSE,warning=FALSE}
#Find Optimal K using Elbow method

wssplot <- function(data, nc=15, seed=1234){
  wss <- (nrow(data)-1)*sum(apply(data,2,var))
  for (i in 2:nc){
    set.seed(seed)
    wss[i] <- sum(kmeans(data, centers=i, nstart=5)$withinss)}
  plot(1:nc, wss, type="b", xlab="Number of Clusters",
       ylab="Within groups sum of squares")
  }

wssplot(df, nc=10) 

```

Based on the Elbow method, we found the optimal number to be 6. Beyond 6, we see gradually decreasing improvements on the SSE.

#### Building K-Means Model 

We are now able to create the K means model, we are creating 4 clusters with the 4 corresponding centroids being randomly chosen 15 times.

```{r, message=FALSE,warning=FALSE}

set.seed(9897665)

kmeans.result <- kmeans(df, centers=6, nstart=15, iter.max = 100)

```

#### Building LM model for each cluster

We now build a linear regression model for each Cluster.

```{r, message=FALSE,warning=FALSE}

data_cluster <- data.frame(data_model,cluster=as.factor(kmeans.result$cluster))
#create blank dataframe to hold all clsuters
lmmodels_output <- data.frame(matrix(ncol = 5, nrow = 0))
x <- c("ClusterNumber", "MinMAxAccuracy", "MAPE","Within10","NumberofRecords")
colnames(lmmodels_output) <- x
for (i in levels(data_cluster$cluster)){
  train_set_c <- createDataPartition(data_cluster$BOROUGH[data_cluster$cluster==i], p = 0.8, list = FALSE)
  #Create train and test data
  train_data_c <- data_model[train_set_c,]
  test_data_C <- data_model[-train_set_c,]
  lmmodels <- train(SALE.PRICE~BLOCK+LOT+LAND.SQUARE.FEET+GROSS.SQUARE.FEET+YEAR.BUILT+BOROUGH+NEIGHBORHOOD+BUILDING.CLASS.CATEGORY +ZIP.CODE+data_out.SALE.DATE , data=train_data, method="lm",tuneLength = 50,
metric = "RMSE",
trControl = fitControl)
  LM_model.pred <- predict(LM_model, test_data_C)

  actuals_preds <- data.frame(cbind(actuals=test_data_C$SALE.PRICE, predicteds=LM_model.pred )) 
  # Check Accuracy of Model 
  min_max_accuracy <- mean (apply(actuals_preds, 1, min) / apply(actuals_preds, 1, max))
  mape <- mean(abs((actuals_preds$predicteds - actuals_preds$actuals))/actuals_preds$actuals) 
  #Within 10%
  Within10 <- ifelse(abs((actuals_preds$predicteds - actuals_preds$actuals))/actuals_preds$actuals>0.1,0,1)
  
  current_output <- data.frame(ClusterNumber = i,MinMAxAccuracy = min_max_accuracy,MAPE = mape,Within10 = mean(Within10) ,NumberofRecords = NROW(train_data))
  
  lmmodels_output <- rbind(lmmodels_output,current_output)
  
  
}

#Find Weighting Scores


lmmodels_output$WeightedWithin10 <- lmmodels_output$Within10 * lmmodels_output$NumberofRecords

#Weight Within10 Accuracy
Clustered_LM_Accuracy <- sum(lmmodels_output$WeightedWithin10) / sum(lmmodels_output$NumberofRecords)

basic_LM_Accuracy
Clustered_LM_Accuracy
round((Clustered_LM_Accuracy / basic_LM_Accuracy)  - 1,4)*100


```

We can see that running the linear regression model on the individual clusters increased the accuracy by `r round((Clustered_LM_Accuracy / basic_LM_Accuracy)  - 1,4)*100` %.


